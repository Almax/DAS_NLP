{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание №1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тема: Языковое моделирование и определение языка.\n",
    "\n",
    "\n",
    "**Выдана**:   14 сентября 2017\n",
    "\n",
    "**Дедлайн**:   <font color='red'>9:00 утра 28 сентября 2017</font>\n",
    "\n",
    "**Среда выполнения**: Jupyter Notebook (Python 3)\n",
    "\n",
    "#### Правила:\n",
    "\n",
    "Результат выполнения задания $-$ отчет в формате Jupyter Notebook с кодом и выводами. В ходе выполнения задания требуется реализовать все необходимые алгоритмы, провести эксперименты и ответить на поставленные вопросы. Дополнительные выводы приветствуются. Чем меньше кода и больше комментариев $-$ тем лучше.\n",
    "\n",
    "Все ячейки должны быть \"выполненными\", при этом результат должен воспроизвдиться при проверке (на Python 3). Если какой-то код не был запущен или отрабатывает с ошибками, то пункт не засчитывается. Задание, сданное после дедлайна, _не принимается_. Совсем.\n",
    "\n",
    "\n",
    "Задание выполняется самостоятельно. Вы можете обсуждать идеи, объяснять друг другу материал, но не можете обмениваться частями своего кода. Если какие-то студенты будут уличены в списывании, все они автоматически получат за эту работу 0 баллов, а также предвзято негативное отношение семинаристов в будущем. Если вы нашли в Интернете какой-то код, который собираетесь заимствовать, обязательно укажите это в задании: вполне вероятно, что вы не единственный, кто найдёт и использует эту информацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Постановка задачи:\n",
    "\n",
    "В данной лабораторной работе Вам предстоит реализовать n-грамную языковую модель с несколькими видами сглаживания:\n",
    "- Add-one smoothing\n",
    "- Stupid backoff\n",
    "- Interpolation smoothing\n",
    "- Kneser-Ney smoothing\n",
    "\n",
    "Вы обучите ее на готовых корпусах, оцените качество и проведете ряд экспериментов. Во второй части задания Вы примените реализованную модель (но с буквенными n-граммами) к задаче распознавания языка. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель языкового моделирования заключается в том, чтобы присвоить некоторые вероятности предложениям. Задача состоит в подсчете вероятности $P(W) = P(w_1, \\dots, w_n)$ или $P(w_n \\mid w_1, \\dots, w_{n-1})$. Модель, умеющая вычислять хотя бы одну из этих двух вероятностей, называется **языковой моделью** (LM от Language Model).\n",
    "\n",
    "Согласно **цепному правилу** (chain rule):\n",
    "\n",
    "$$P(X_1, \\dots, X_n) = P(X_1)P(X_2 \\mid X_1)\\dots P(X_n \\mid X_1, \\dots, X_{n-1}).$$ \n",
    "\n",
    "Также мы знаем, что\n",
    "\n",
    "$$\n",
    "    P(X_n \\mid X_1, \\dots, X_{n-1}) = \\frac{P(X_1, \\dots, X_n)}{P(X_1, \\dots, X_{n-1})},\n",
    "$$\n",
    "\n",
    "следовательно, для того чтобы оценить $P(X_n \\mid X_1, \\dots, X_{n-1})$ нужно посчитать $P(X_1, \\dots, X_n)$ и $P(X_1, \\dots, X_{n-1})$. Но эти вероятности будут чрезвычайно малы, если мы возьмем большое $n$, так множество предложений из $n$ слов растет экспоненциально. Для упрощения применим **марковское предположение**: \n",
    "\n",
    "$$P(X_n \\mid X_1, \\dots, X_{n-1}) = P(X_n \\mid X_{n - k + 1}, \\dots, X_{n-1})$$\n",
    "\n",
    "для некоторого фиксированного (небольшого) $k$. Это предположение говорит о том, что $X_{n}$ не зависит от $X_{1}, \\dots, X_{n - k}$, то есть на следующее слово влияет лишь контекст из предыдущих $k - 1$ слова. Таким образом, мы получаем финальную вероятность:\n",
    "\n",
    "$$\n",
    "    P(w_1, \\dots, w_n) = \\prod_i P(w_i \\mid w_{i-k+1}, \\dots, w_{i - 1}).\n",
    "$$\n",
    "\n",
    "Далее для краткости будем обозначать $w_{i-k}^i := w_{i-k}, \\dots, w_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Хранилище n-грам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала выполним вспомогательную работу. Следуйте комментариям, чтобы написать NGramStorage с удобным интерфейсом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramStorage:\n",
    "    \"\"\"Storage for ngrams' frequencies.\n",
    "\n",
    "    Args:\n",
    "        sents (list[list[str]]): List of sentences from which ngram\n",
    "            frequencies are extracted.\n",
    "        max_n (int): Upper bound of the length of ngrams.\n",
    "            For instance if max_n = 2, then storage will store\n",
    "            0, 1, 2-grams.\n",
    "\n",
    "    Attributes:\n",
    "        max_n (Readonly(int)): Upper bound of the length of ngrams.\n",
    "    \"\"\"\n",
    "\n",
    "    def __make_lower(self, n_gram):\n",
    "        return tuple([word.lower() for word in n_gram])\n",
    "\n",
    "    def __init__(self, sents=[], max_n=0):\n",
    "        self.__max_n = max_n\n",
    "        self.__ngrams = {i: Counter() for i in range(self.__max_n + 1)}\n",
    "        # self._ngrams[K] should have the following interface:\n",
    "        # self._ngrams[K][(w_1, ..., w_K)] = number of times w_1, ..., w_K occured in words\n",
    "        # self._ngrams[0][()] = number of all words\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        def handle_sentence(sentence):\n",
    "            for length in range(1, max_n + 1):\n",
    "                for i in range(len(sentence)):\n",
    "                    new_n_gram = tuple(sentence[i:(i + length)])\n",
    "                    if len(new_n_gram) == length:\n",
    "                        self.__ngrams[length][new_n_gram] += 1\n",
    "                        if length == 1:\n",
    "                            self.__ngrams[0][tuple()] += 1\n",
    "        for sentence in sents:\n",
    "            handle_sentence(self.__make_lower(sentence))\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "    def add_unk_token(self):\n",
    "        \"\"\"Add UNK token to 1-grams.\"\"\"\n",
    "        # In order to avoid zero probabilites \n",
    "        if self.__max_n == 0 or 'UNK' in self.__ngrams[1]:\n",
    "            return\n",
    "        self.__ngrams[0][()] += 1\n",
    "        self.__ngrams[1][('UNK', )] = 1\n",
    "        \n",
    "    @property\n",
    "    def max_n(self):\n",
    "        \"\"\"Get max_n\"\"\"\n",
    "        return self.__max_n\n",
    "        \n",
    "    def __getitem__(self, k):\n",
    "        \"\"\"Get dictionary of k-gram frequencies.\n",
    "        \n",
    "        Args:\n",
    "            k (int): length of returning ngrams' frequencies.\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary (in fact Counter) of k-gram frequencies.\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(k, int):\n",
    "            raise TypeError('k (length of ngrams) must be an integer!')\n",
    "        if k > self.__max_n:\n",
    "            raise ValueError('k (length of ngrams) must be less or equal to the maximal length!')\n",
    "        return self.__ngrams[k]\n",
    "    \n",
    "    def __call__(self, ngram):\n",
    "        \"\"\"Return frequency of a given ngram.\n",
    "        \n",
    "        Args:\n",
    "            ngram (tuple): ngram for which frequency should be computed.\n",
    "            \n",
    "        Returns:\n",
    "            Frequency (int) of a given ngram.\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(ngram, tuple):\n",
    "            raise TypeError('ngram must be a tuple!')\n",
    "        if len(ngram) > self.__max_n:\n",
    "            raise ValueError('length of ngram must be less or equal to the maximal length!')\n",
    "        if len(ngram) == 1 and ngram not in self.__ngrams[1]:\n",
    "            return self.__ngrams[1][('UNK', )]\n",
    "        return self.__ngrams[len(ngram)][self.__make_lower(ngram)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте brown корпус, обучите модель и протестируйте на нескольких примерах последовательностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Uncomment next row and download brown corpus\n",
    "# nltk.download()\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all sentences = 57340\n",
      "Number of train sentences = 45872\n",
      "Number of test sentences = 11468\n"
     ]
    }
   ],
   "source": [
    "all_sents = list(brown.sents())\n",
    "random.shuffle(all_sents)\n",
    "print('Number of all sentences = {}'.format(len(all_sents)))\n",
    "train_sents = all_sents[:int(0.8 * len(all_sents))]\n",
    "test_sents = all_sents[int(0.8 * len(all_sents)):]\n",
    "print('Number of train sentences = {}'.format(len(train_sents)))\n",
    "print('Number of test sentences = {}'.format(len(test_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create storage of 0, 1, 2, 3-grams\n",
    "storage = NGramStorage(train_sents, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1390\n",
      "3387\n",
      "31\n",
      "0\n",
      "931059\n"
     ]
    }
   ],
   "source": [
    "# It's time to test your code\n",
    "print(storage(('to', 'be')))\n",
    "print(storage(('or',)))\n",
    "print(storage(('not', 'to', 'be')))\n",
    "print(storage(('somethingweird',)))\n",
    "print(storage(()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для численного измерения качества языковой модели определим **перплексию**:\n",
    "\n",
    "$$\n",
    "    {\\mathbb{P}}(w_1, \\dots, w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_i P(w_i \\mid w_{i - k}, \\dots, w_{i - 1})\\right)^{-\\frac1N},\n",
    "$$\n",
    "\n",
    "Вижно, что минимизация перплексии эквивалентна максимизации правдоподобия модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте функцию по подсчету перплексии. Обратите внимание, что перплексия по корпусу равна произведению вероятностей **всех** предложений в степени $-\\frac1N$, где $N -$ суммарная длина всех предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(estimator, sents):\n",
    "    '''Estimate perplexity of the sequence of words using prob_estimator.'''\n",
    "    ### YOUR CODE HERE\n",
    "    # Avoid log(0) by replacing zero by 10 ** (-50).\n",
    "    log_perp = 0\n",
    "    N = 0\n",
    "    for sentence in sents:\n",
    "        log_perp += np.log(max(estimator.prob(sentence), 10 ** (-50)))\n",
    "        N += len(sentence)\n",
    "    perp = np.exp(-log_perp / N)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Оценка вероятностей n-грам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый и простейший способ оценки вероятностей N-грам следующий:\n",
    "\n",
    "$$\n",
    "    \\hat P_{S}(w_{N} \\mid w_1^{N - 1}) = \\frac{c(w_1^N)}{c(w_1^{N-1})}.\n",
    "$$\n",
    "\n",
    "где $c(w_1^N)$ — это число последовательностей $w_1, \\dots, w_N$ в корпусе, $S$ символизирует Straightforward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StraightforwardProbabilityEstimator:\n",
    "    \"\"\"Class for simplest probability estimations of type P(word | context).\n",
    "    \n",
    "    P(word | context) = c(context + word) / c(context), where\n",
    "    c(sequence) - number of occurances of the sequence in the corpus.\n",
    "    \n",
    "    Args:\n",
    "        storage(NGramStorage): Object of NGramStorage class which will\n",
    "            be used to extract frequencies of ngrams.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage):\n",
    "        self.__storage = storage\n",
    "        # Adding UNK token to avoid zero probabilities\n",
    "        self.__storage.add_unk_token()\n",
    "        \n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) up to the length of max_n.\n",
    "        \"\"\"\n",
    "        if self.__storage.max_n == 1:\n",
    "            return ()\n",
    "        if len(context) + 1 > self.__storage.max_n:\n",
    "            return context[-self.__storage.max_n + 1:]\n",
    "        return context\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(word, str):\n",
    "            raise TypeError('word must be a string!')\n",
    "        if not isinstance(context, tuple):\n",
    "            raise TypeError('word must be a string!')\n",
    "        # If context is too large, let's cut it.\n",
    "        context = self.cut_context(context)\n",
    "        phrase_counts = self.__storage(context + (word, ))\n",
    "        context_counts = self.__storage(context)\n",
    "        # Avoiding 0 / 0.\n",
    "        if context_counts == 0:\n",
    "            return 0.\n",
    "        return 1. * phrase_counts / context_counts\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple estimator perplexity = 260.74327627856337\n",
      "0.001492922045840225\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "simple_estimator = StraightforwardProbabilityEstimator(storage)\n",
    "\n",
    "# Estimating perplexity\n",
    "print('Simple estimator perplexity = {}'.format(perplexity(simple_estimator, test_sents)))\n",
    "print(simple_estimator.prob('To be'.split()))\n",
    "print(simple_estimator.prob('To be or not to be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем перплексию униграмной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple estimator perplexity = 121.12407024509638\n"
     ]
    }
   ],
   "source": [
    "uni_storage = NGramStorage(train_sents, 1)\n",
    "uni_simple_estimator = StraightforwardProbabilityEstimator(uni_storage)\n",
    "print('Simple estimator perplexity = {}'.format(perplexity(uni_simple_estimator, test_sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на следующие вопросы (внутри ipython ноутбука):\n",
    "\n",
    "**Q:** Какие выводы можно сделать? Почему $P(\\text{To be or not to be}) = 0$, хотя мы и добавили UNK токен?  \n",
    "**A:** Вывод: казалось бы, более умная, учитывающая больше признаков модель, а работает хуже. Это из-за дырок, которые описаны в следующем ответе. В триграммной модели они проявляются гораздо чаще, чем в униграммной, т.к. число различных приграмм в языке сильно превышает число различных униграмм.\n",
    "\n",
    "**Q:** Почему перплексия униграмной модели меньше, чем триграмной?  \n",
    "**A:** В триграмной модели появляются дырки, т.е. не присутствующие в словаре последовательности токенов, частота которых 0. Из-за этого резко уменьшается перплексия.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add-one smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простейший вид сглаживания — **сглаживание Лапласа**. Чтобы избавиться от нулевых вероятностей $P(w_{N} \\mid w_1^{N - 1})$, будем использовать формулу:\n",
    "\n",
    "$$\n",
    "    \\hat P_{AOS}(w_{N} \\mid w_1^{N - 1}) = \\frac{c(w_1^N) + \\delta}{c(w_1^{N-1}) + \\delta V},\n",
    "$$\n",
    "\n",
    "где $V$ — это размер словаря, а $\\delta$ — некоторая фиксированная константа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте класс, осуществляющий сглаживание Лапласа. Он должен иметь аналогичный интерфейс, как и StraightforwardProbabilityEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LaplaceProbabilityEstimator:\n",
    "    \"\"\"Class for probability estimations of type P(word | context).\n",
    "    \n",
    "    P(word | context) = (c(context + word) + delta) / (c(context) + delta * V), where\n",
    "    c(sequence) - number of occurances of the sequence in the corpus,\n",
    "    delta - some constant,\n",
    "    V - number of different words in corpus.\n",
    "    \n",
    "    Args:\n",
    "        storage(NGramStorage): Object of NGramStorage class which will\n",
    "            be used to extract frequencies of ngrams.\n",
    "        delta(float): Smoothing parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage, delta=1.):\n",
    "        self.__storage = storage\n",
    "        self.__delta = delta\n",
    "        \n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) up to the length of max_n.\n",
    "        \"\"\"\n",
    "        if len(context) + 1 > self.__storage.max_n:\n",
    "            return context[-self.__storage.max_n + 1:]\n",
    "        return context\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(word, str):\n",
    "            raise TypeError('word must be a string!')\n",
    "        if not isinstance(context, tuple):\n",
    "            raise TypeError('context must be a tuple!')\n",
    "            \n",
    "        ### YOUR CODE HERE\n",
    "        # If context is too large, let's cut it.\n",
    "        context = self.cut_context(context)\n",
    "        phrase_counts = self.__storage(context + (word, ))\n",
    "        context_counts = self.__storage(context)\n",
    "        # Avoiding 0 / 0.\n",
    "        if context_counts == 0:\n",
    "            return 0.\n",
    "        prob = (phrase_counts + self.__delta) / (context_counts + self.__delta * len(self.__storage[1]))\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберите наилучший параметр $\\delta$ для данного корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace estimator perplexity = 215.7149646790965\n",
      "0.00045824745623\n"
     ]
    }
   ],
   "source": [
    "# Try to find out best delta parameter. We will not provide you any strater code.\n",
    "### YOUR CODE HERE\n",
    "def get_perplexity(delta):\n",
    "    return perplexity(LaplaceProbabilityEstimator(storage, delta), test_sents)\n",
    "\n",
    "deltas = np.linspace(0, 0.00005, 100)\n",
    "perplexities = np.array([get_perplexity(delta) for delta in deltas])\n",
    "best_delta = deltas[np.argmin(perplexities)]\n",
    "### END YOUR CODE\n",
    "\n",
    "# Initialize estimator\n",
    "laplace_estimator = LaplaceProbabilityEstimator(storage, best_delta)\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Laplace estimator perplexity = {}'.format(perplexity(laplace_estimator, test_sents)))\n",
    "print(laplace_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fb414de5630>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb4176aaac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plot\n",
    "%matplotlib inline\n",
    "\n",
    "# Рисуем, чтобы удостовериться, что выбран правильный интервал поиска дельты\n",
    "axis = plot.gca()\n",
    "plot.plot(deltas, perplexities)\n",
    "axis.set_xlabel('delta')\n",
    "axis.set_ylabel('perplexy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stupid backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **простого отката** довольно понятна. Если у нас есть достаточно информцаии для подсчета вероятности $k$-грам, то будем использовать $k$-грамы. Иначе будем использовать вероятности $(k-1)$-грам с некоторым множителем, например, $0.4$, и так далее. К сожалению, в данном случае мы получим не вероятностное распределение, но в большинстве задач это не имеет принципиального значения. Если это все же важно, то необходимо подобрать множитель соответствующим образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте класс, симулирующий сглаживание простым откатом. Он должен иметь аналогичный интерфейс, как и StraightforwardProbabilityEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StupidBackoffProbabilityEstimator:\n",
    "    \"\"\"Class for stupid backoff probability estimations.\n",
    "    \n",
    "    P(word | context) =\n",
    "        P'(word | context),                  if  P'(word | context) > 0;\n",
    "        P'(word | context[1:]) * multiplier, if  P'(word | context) == 0\n",
    "                                             and P'(word | context[1:]) > 0;\n",
    "        ...\n",
    "    P'(word | context) - probability of a word provided context of a base estimator.\n",
    "    \n",
    "    Args:\n",
    "        base_estimator(BaseProbabilityEstimator): Object of BaseProbabilityEstimator\n",
    "            or some other class which can estimate conditional probabilities.\n",
    "        multiplier (float): Multiplier which is used for probability estimations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_estimator, multiplier=0.1):\n",
    "        self.__base_estimator = base_estimator\n",
    "        self.__mult = multiplier\n",
    "\n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) by one word.\n",
    "        \"\"\"\n",
    "        if len(context) != 1:\n",
    "            return context[1:]\n",
    "        else:\n",
    "            return tuple()\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        counter = 0\n",
    "        while self.__base_estimator(word, context) == 0:\n",
    "            context = self.cut_context(context)\n",
    "            counter += 1\n",
    "        prob = (self.__mult ** counter) * self.__base_estimator(word, context)\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stupid backoff estimator perplexity = 120.18020894160107\n",
      "0.00045824745623\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "sbackoff_estimator = StupidBackoffProbabilityEstimator(simple_estimator, .4)\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Stupid backoff estimator perplexity = {}'.format(perplexity(sbackoff_estimator, test_sents)))\n",
    "print(laplace_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на следующие вопросы (внутри ipython ноутбука):\n",
    "\n",
    "**Q:** Почему бессмысленно измерять перплексию в случае **Stupid backoff**?  \n",
    "**A:** Из-за тупого отнятия вероятности распределение перестает быть вероятностным (сумма всех вероятностей не единица). Перплексия, как вероятностная величина теряет значение.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае идея сглаживания посредством **интерполяции** также крайне проста. Пусть у нас есть $N$-грамная модель. Заведем вектор $\\bar\\lambda = (\\lambda_1, \\dots, \\lambda_N)$, такой, что $\\sum_i\\lambda_i = 1$ и $\\lambda_i \\geq 0$. Тогда\n",
    "\n",
    "$$\n",
    "    \\hat P_{IS}(w_{N} \\mid w_1^{N-1}) = \\sum_{i=1}^N \\lambda_i \\hat P_{S}(w_N \\mid w_{N-i+1}^{N-1}).\n",
    "$$\n",
    "\n",
    "Придумайте, как обойтись одним вектором $\\bar\\lambda$, т.е. пользоваться им как в случае контекста длины $N$, так и при контексте меньшей длины (например, в начале предложения). Если мы просто обрубим сумму, то у нас уже не будет вероятностное распределение, что, конечно же, плохо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InterpolationProbabilityEstimator:\n",
    "    \"\"\"Class for interpolation probability estimations.\n",
    "    \n",
    "    P(word | context) =\n",
    "        lambda_N * P'(word | context) +\n",
    "        lambda_{N-1} * P'(word | context[1:]) +\n",
    "        ... +\n",
    "        lambda_1 * P'(word)\n",
    "    P'(word | context) - probability of a word provided context of a base estimator.\n",
    "    \n",
    "    Args:\n",
    "        base_estimator(BaseProbabilityEstimator): Object of BaseProbabilityEstimator\n",
    "            or some other class which can estimate conditional probabilities.\n",
    "        lambdas (np.array[float]): Lambdas which are used for probability estimations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_estimator, lambdas):\n",
    "        self.lambdas = lambdas\n",
    "        self.__base_estimator = base_estimator\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        def normalized(lambdas):\n",
    "            lambdas_sum = sum(lambdas)\n",
    "            return np.array([one_lambda / lambdas_sum for one_lambda in lambdas])\n",
    "        # Обрезаем лямбды в начале. Делаем так, чтобы их было на 1 больше, чем длина контекста.\n",
    "        current_lambdas = normalized(self.lambdas[(len(self.lambdas) - len(context) - 1):])\n",
    "        prob = np.sum([\n",
    "            one_lambda * self.__base_estimator(word, context[i:])\n",
    "            for i, one_lambda in enumerate(reversed(current_lambdas))\n",
    "        ])\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation estimator perplexity = 234.54860365993025\n",
      "0.00045824745623\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize estimator\n",
    "interpol_estimator = InterpolationProbabilityEstimator(simple_estimator, np.array([0.2, 0.2, 0.6]))\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Interpolation estimator perplexity = {}'.format(perplexity(interpol_estimator, test_sents)))\n",
    "print(laplace_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучить значения параметров $\\lambda$ можно с помощью EM-алгоритма, но мы не будем этого здесь делать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kneser-Ney smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея данного сглаживания заключается в том, что словам, которые участвуют в большом количестве контекстов, присваиваются большие вероятности, а те, которые используются в паре-тройке контекстов, получают маленькие вероятности. Формулы приведены на слайде 37 лекции.\n",
    "Реализуйте данный подход."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KneserNeyProbabilityEstimator:\n",
    "    \"\"\"Class for probability estimations of type P(word | context).\n",
    "    \n",
    "    P(word | context) = ...\n",
    "    \n",
    "    Args:\n",
    "        storage(NGramStorage): Object of NGramStorage class which will\n",
    "            be used to extract frequencies of ngrams.\n",
    "        delta(float): KneserNey parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage, delta=1.):\n",
    "        self.__storage = storage\n",
    "        self.__delta = delta\n",
    "        \n",
    "        self._unique_continuations_number = {}\n",
    "        self._continuations_number = {}\n",
    "        self._bigramms_beginigs_number = {}\n",
    "        counter = 0\n",
    "        for sentence in self.__storage[2]:\n",
    "            counter += self.__storage[2][sentence]\n",
    "        self._bigramms_number = counter\n",
    "    \n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) up to the length of max_n.\n",
    "        \"\"\"\n",
    "        if len(context) + 1 > self.__storage.max_n:\n",
    "            context = context[-self.__storage.max_n + 1:]\n",
    "        return context\n",
    "\n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(word, str):\n",
    "            raise TypeError('word must be a string!')\n",
    "        if not isinstance(context, tuple):\n",
    "            raise TypeError('word must be a string!')\n",
    "        # If context is too large, let's cut it.\n",
    "        context = self.cut_context(context)\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        def get_unique_continuations_number(context):\n",
    "            if context not in self._unique_continuations_number:\n",
    "                counter = 0\n",
    "                for sentence in self.__storage[len(context) + 1]:\n",
    "                    if sentence[:-1] == context:\n",
    "                        counter += 1\n",
    "                self._unique_continuations_number[context] = counter\n",
    "            return self._unique_continuations_number[context]\n",
    "\n",
    "        def get_continuations_number(context):\n",
    "            if context not in self._continuations_number:\n",
    "                counter = 0\n",
    "                for sentence in self.__storage[len(context) + 1]:\n",
    "                    if sentence[:-1] == context:\n",
    "                        counter += self.__storage(sentence)\n",
    "                self._continuations_number[context] = counter\n",
    "            return self._continuations_number[context]\n",
    "        \n",
    "        def get_bigramms_beginigs_number(word):\n",
    "            if word not in self._bigramms_beginigs_number:\n",
    "                counter = 0\n",
    "                for bigram in self.__storage[2]:\n",
    "                    if bigram[1] == word:\n",
    "                        counter += 1\n",
    "                self._bigramms_beginigs_number[word] = counter\n",
    "            return self._bigramms_beginigs_number[word]\n",
    "        \n",
    "        phrase_counts = self.__storage(context + (word, ))\n",
    "        context_counts = get_continuations_number(context)\n",
    "        if context_counts == 0:\n",
    "            prob = self(word, context[1:])\n",
    "        elif len(context) == 0:\n",
    "            prob = get_bigramms_beginigs_number(word) / self._bigramms_number\n",
    "        else:\n",
    "            prob = (max(phrase_counts - self.__delta, 0) / context_counts\n",
    "                    + self.__delta * get_unique_continuations_number(context) * self(word, context[1:]) / context_counts)\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize estimator\n",
    "kn_estimator = KneserNeyProbabilityEstimator(storage)\n",
    "\n",
    "# Estimating perplexity\n",
    "print('Simple estimator perplexity = {}'.format(perplexity(kn_estimator, test_sents)))\n",
    "print(simple_estimator.prob('To be'.split()))\n",
    "print(simple_estimator.prob('To be or not to be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определение языка документа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Постановка задачи:**  \n",
    "Одна из задач, которая может быть решена при помощи языковых моделей $-$ **определение языка документа**. Реализуйте два классификатора для определения языка документа:\n",
    "1. Наивный классификатор, который будет учитывать частоты символов и выбирать язык текста по признаку: распределение частот символов \"наиболее похоже\" на распределение частот символов в выбранном языке.\n",
    "2. Классификатор на основе языковых моделей. Сами придумайте, как он должен работать.  \n",
    "_Подсказка_: лучше считать n-грамы не по словам, а по символам.\n",
    "\n",
    "---\n",
    "\n",
    "**Как представлены данные:**  \n",
    "Во всех текстовых файлах на каждой строчке записано отдельное предложение.\n",
    "1. В папке _data_ находятся две папки: _full_ и _plain_. В _full_ находятся тексты в той форме, что они были взяты из сети, в _plain_ находятся те же самые тексты, но с них сначала была снята диакритика, а затем русский и греческий тексты были транслитерованы в английский.\n",
    "2. В каждой из папок _full_ и _plain_ находятся папки _train_ и _test_.\n",
    "3. В _train_ находятся файлы с текстами с говорящими именами, например, _ru.txt_, _en.txt_.\n",
    "4. В _test_ находятся файлы _1.txt_, _2.txt_, $\\dots$ в которых хранятся тексты, язык которых нужно определить. В этой же папке находится файл _ans.csv_, в котором вы можете найти правильные ответы и проверить, насколько хорошо сработали Ваши алгоритмы.\n",
    "\n",
    "---\n",
    "\n",
    "**Что нужно сделать:**  \n",
    "Напишите два своих классификатора (которые описаны в постановке задачи) и получите максимально возможное accuracy на test-сете. Разрешается использовать только _train_ для обучения.\n",
    "\n",
    "---\n",
    "\n",
    "**В данном задании мы не предоставляем стартового кода!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the code and estimate accuracy of your method.\n",
    "# Create your own classifiers.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
