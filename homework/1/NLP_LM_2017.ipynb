{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание №1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тема: Языковое моделирование и определение языка.\n",
    "\n",
    "\n",
    "**Выдана**:   14 сентября 2017\n",
    "\n",
    "**Дедлайн**:   <font color='red'>9:00 утра 28 сентября 2017</font>\n",
    "\n",
    "**Среда выполнения**: Jupyter Notebook (Python 3)\n",
    "\n",
    "#### Правила:\n",
    "\n",
    "Результат выполнения задания $-$ отчет в формате Jupyter Notebook с кодом и выводами. В ходе выполнения задания требуется реализовать все необходимые алгоритмы, провести эксперименты и ответить на поставленные вопросы. Дополнительные выводы приветствуются. Чем меньше кода и больше комментариев $-$ тем лучше.\n",
    "\n",
    "Все ячейки должны быть \"выполненными\", при этом результат должен воспроизвдиться при проверке (на Python 3). Если какой-то код не был запущен или отрабатывает с ошибками, то пункт не засчитывается. Задание, сданное после дедлайна, _не принимается_. Совсем.\n",
    "\n",
    "\n",
    "Задание выполняется самостоятельно. Вы можете обсуждать идеи, объяснять друг другу материал, но не можете обмениваться частями своего кода. Если какие-то студенты будут уличены в списывании, все они автоматически получат за эту работу 0 баллов, а также предвзято негативное отношение семинаристов в будущем. Если вы нашли в Интернете какой-то код, который собираетесь заимствовать, обязательно укажите это в задании: вполне вероятно, что вы не единственный, кто найдёт и использует эту информацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Постановка задачи:\n",
    "\n",
    "В данной лабораторной работе Вам предстоит реализовать n-грамную языковую модель с несколькими видами сглаживания:\n",
    "- Add-one smoothing\n",
    "- Stupid backoff\n",
    "- Interpolation smoothing\n",
    "- Kneser-Ney smoothing\n",
    "\n",
    "Вы обучите ее на готовых корпусах, оцените качество и проведете ряд экспериментов. Во второй части задания Вы примените реализованную модель (но с буквенными n-граммами) к задаче распознавания языка. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель языкового моделирования заключается в том, чтобы присвоить некоторые вероятности предложениям. Задача состоит в подсчете вероятности $P(W) = P(w_1, \\dots, w_n)$ или $P(w_n \\mid w_1, \\dots, w_{n-1})$. Модель, умеющая вычислять хотя бы одну из этих двух вероятностей, называется **языковой моделью** (LM от Language Model).\n",
    "\n",
    "Согласно **цепному правилу** (chain rule):\n",
    "\n",
    "$$P(X_1, \\dots, X_n) = P(X_1)P(X_2 \\mid X_1)\\dots P(X_n \\mid X_1, \\dots, X_{n-1}).$$ \n",
    "\n",
    "Также мы знаем, что\n",
    "\n",
    "$$\n",
    "    P(X_n \\mid X_1, \\dots, X_{n-1}) = \\frac{P(X_1, \\dots, X_n)}{P(X_1, \\dots, X_{n-1})},\n",
    "$$\n",
    "\n",
    "следовательно, для того чтобы оценить $P(X_n \\mid X_1, \\dots, X_{n-1})$ нужно посчитать $P(X_1, \\dots, X_n)$ и $P(X_1, \\dots, X_{n-1})$. Но эти вероятности будут чрезвычайно малы, если мы возьмем большое $n$, так множество предложений из $n$ слов растет экспоненциально. Для упрощения применим **марковское предположение**: \n",
    "\n",
    "$$P(X_n \\mid X_1, \\dots, X_{n-1}) = P(X_n \\mid X_{n - k + 1}, \\dots, X_{n-1})$$\n",
    "\n",
    "для некоторого фиксированного (небольшого) $k$. Это предположение говорит о том, что $X_{n}$ не зависит от $X_{1}, \\dots, X_{n - k}$, то есть на следующее слово влияет лишь контекст из предыдущих $k - 1$ слова. Таким образом, мы получаем финальную вероятность:\n",
    "\n",
    "$$\n",
    "    P(w_1, \\dots, w_n) = \\prod_i P(w_i \\mid w_{i-k+1}, \\dots, w_{i - 1}).\n",
    "$$\n",
    "\n",
    "Далее для краткости будем обозначать $w_{i-k}^i := w_{i-k}, \\dots, w_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Хранилище n-грам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала выполним вспомогательную работу. Следуйте комментариям, чтобы написать NGramStorage с удобным интерфейсом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramStorage:\n",
    "    \"\"\"Storage for ngrams' frequencies.\n",
    "\n",
    "    Args:\n",
    "        sents (list[list[str]]): List of sentences from which ngram\n",
    "            frequencies are extracted.\n",
    "        max_n (int): Upper bound of the length of ngrams.\n",
    "            For instance if max_n = 2, then storage will store\n",
    "            0, 1, 2-grams.\n",
    "\n",
    "    Attributes:\n",
    "        max_n (Readonly(int)): Upper bound of the length of ngrams.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sents=[], max_n=0):\n",
    "        self.__max_n = max_n\n",
    "        self.__ngrams = {i: Counter() for i in range(self.__max_n + 1)}\n",
    "        # self._ngrams[K] should have the following interface:\n",
    "        # self._ngrams[K][(w_1, ..., w_K)] = number of times w_1, ..., w_K occured in words\n",
    "        # self._ngrams[0][()] = number of all words\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        def handle_sentence(sentence):\n",
    "            for length in range(1, max_n + 1):\n",
    "                for i in range(len(sentence)):\n",
    "                    new_n_gram = tuple(sentence[i:(i + length)])\n",
    "                    if len(new_n_gram) == length:\n",
    "                        self.__ngrams[length][new_n_gram] += 1\n",
    "                        if length == 1:\n",
    "                            self.__ngrams[0][tuple()] += 1\n",
    "        for sentence in sents:\n",
    "            handle_sentence(sentence)\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "    def add_unk_token(self):\n",
    "        \"\"\"Add UNK token to 1-grams.\"\"\"\n",
    "        # In order to avoid zero probabilites \n",
    "        if self.__max_n == 0 or 'UNK' in self.__ngrams[1]:\n",
    "            return\n",
    "        self.__ngrams[0][()] += 1\n",
    "        self.__ngrams[1][('UNK', )] = 1\n",
    "        \n",
    "    @property\n",
    "    def max_n(self):\n",
    "        \"\"\"Get max_n\"\"\"\n",
    "        return self.__max_n\n",
    "        \n",
    "    def __getitem__(self, k):\n",
    "        \"\"\"Get dictionary of k-gram frequencies.\n",
    "        \n",
    "        Args:\n",
    "            k (int): length of returning ngrams' frequencies.\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary (in fact Counter) of k-gram frequencies.\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(k, int):\n",
    "            raise TypeError('k (length of ngrams) must be an integer!')\n",
    "        if k > self.__max_n:\n",
    "            raise ValueError('k (length of ngrams) must be less or equal to the maximal length!')\n",
    "        return self.__ngrams[k]\n",
    "    \n",
    "    def __call__(self, ngram):\n",
    "        \"\"\"Return frequency of a given ngram.\n",
    "        \n",
    "        Args:\n",
    "            ngram (tuple): ngram for which frequency should be computed.\n",
    "            \n",
    "        Returns:\n",
    "            Frequency (int) of a given ngram.\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(ngram, tuple):\n",
    "            raise TypeError('ngram must be a tuple!')\n",
    "        if len(ngram) > self.__max_n:\n",
    "            raise ValueError('length of ngram must be less or equal to the maximal length!')\n",
    "        if len(ngram) == 1 and ngram not in self.__ngrams[1]:\n",
    "            return self.__ngrams[1][('UNK', )]\n",
    "        return self.__ngrams[len(ngram)][ngram]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте brown корпус, обучите модель и протестируйте на нескольких примерах последовательностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Uncomment next row and download brown corpus\n",
    "# nltk.download()\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all sentences = 57340\n",
      "Number of train sentences = 45872\n",
      "Number of test sentences = 11468\n"
     ]
    }
   ],
   "source": [
    "all_sents = list(brown.sents())\n",
    "random.shuffle(all_sents)\n",
    "print('Number of all sentences = {}'.format(len(all_sents)))\n",
    "train_sents = all_sents[:int(0.8 * len(all_sents))]\n",
    "test_sents = all_sents[int(0.8 * len(all_sents)):]\n",
    "print('Number of train sentences = {}'.format(len(train_sents)))\n",
    "print('Number of test sentences = {}'.format(len(test_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create storage of 0, 1, 2, 3-grams\n",
    "storage = NGramStorage(train_sents, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1355\n",
      "3358\n",
      "27\n",
      "0\n",
      "930105\n"
     ]
    }
   ],
   "source": [
    "# It's time to test your code\n",
    "print(storage(('to', 'be')))\n",
    "print(storage(('or',)))\n",
    "print(storage(('not', 'to', 'be')))\n",
    "print(storage(('somethingweird',)))\n",
    "print(storage(()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для численного измерения качества языковой модели определим **перплексию**:\n",
    "\n",
    "$$\n",
    "    {\\mathbb{P}}(w_1, \\dots, w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_i P(w_i \\mid w_{i - k}, \\dots, w_{i - 1})\\right)^{-\\frac1N},\n",
    "$$\n",
    "\n",
    "Вижно, что минимизация перплексии эквивалентна максимизации правдоподобия модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте функцию по подсчету перплексии. Обратите внимание, что перплексия по корпусу равна произведению вероятностей **всех** предложений в степени $-\\frac1N$, где $N -$ суммарная длина всех предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(estimator, sents):\n",
    "    '''Estimate perplexity of the sequence of words using prob_estimator.'''\n",
    "    ### YOUR CODE HERE\n",
    "    # Avoid log(0) by replacing zero by 10 ** (-50).\n",
    "    log_perp = 0\n",
    "    N = 0\n",
    "    for sentence in sents:\n",
    "        log_perp += np.log(max(estimator.prob(sentence), 10 ** (-50)))\n",
    "        N += len(sentence)\n",
    "    perp = np.exp(-log_perp / N)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Оценка вероятностей n-грам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый и простейший способ оценки вероятностей N-грам следующий:\n",
    "\n",
    "$$\n",
    "    \\hat P_{S}(w_{N} \\mid w_1^{N - 1}) = \\frac{c(w_1^N)}{c(w_1^{N-1})}.\n",
    "$$\n",
    "\n",
    "где $c(w_1^N)$ — это число последовательностей $w_1, \\dots, w_N$ в корпусе, $S$ символизирует Straightforward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StraightforwardProbabilityEstimator:\n",
    "    \"\"\"Class for simplest probability estimations of type P(word | context).\n",
    "    \n",
    "    P(word | context) = c(context + word) / c(context), where\n",
    "    c(sequence) - number of occurances of the sequence in the corpus.\n",
    "    \n",
    "    Args:\n",
    "        storage(NGramStorage): Object of NGramStorage class which will\n",
    "            be used to extract frequencies of ngrams.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage):\n",
    "        self.__storage = storage\n",
    "        # Adding UNK token to avoid zero probabilities\n",
    "        self.__storage.add_unk_token()\n",
    "        \n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) up to the length of max_n.\n",
    "        \"\"\"\n",
    "        if self.__storage.max_n == 1:\n",
    "            return ()\n",
    "        if len(context) + 1 > self.__storage.max_n:\n",
    "            return context[-self.__storage.max_n + 1:]\n",
    "        return context\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(word, str):\n",
    "            raise TypeError('word must be a string!')\n",
    "        if not isinstance(context, tuple):\n",
    "            raise TypeError('word must be a string!')\n",
    "        # If context is too large, let's cut it.\n",
    "        context = self.cut_context(context)\n",
    "        phrase_counts = self.__storage(context + (word, ))\n",
    "        context_counts = self.__storage(context)\n",
    "        # Avoiding 0 / 0.\n",
    "        if context_counts == 0:\n",
    "            return 0.\n",
    "        return 1. * phrase_counts / context_counts\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple estimator perplexity = 258.2879746692547\n",
      "1.3976901557456893e-05\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "simple_estimator = StraightforwardProbabilityEstimator(storage)\n",
    "\n",
    "# Estimating perplexity\n",
    "print('Simple estimator perplexity = {}'.format(perplexity(simple_estimator, test_sents)))\n",
    "print(simple_estimator.prob('To be'.split()))\n",
    "print(simple_estimator.prob('To be or not to be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем перплексию униграмной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple estimator perplexity = 105.26211154007639\n"
     ]
    }
   ],
   "source": [
    "uni_storage = NGramStorage(train_sents, 1)\n",
    "uni_simple_estimator = StraightforwardProbabilityEstimator(uni_storage)\n",
    "print('Simple estimator perplexity = {}'.format(perplexity(uni_simple_estimator, test_sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на следующие вопросы (внутри ipython ноутбука):\n",
    "\n",
    "**Q:** Какие выводы можно сделать? Почему $P(\\text{To be or not to be}) = 0$, хотя мы и добавили UNK токен?  \n",
    "**A:** Вывод: казалось бы, более умная, учитывающая больше признаков модель, а работает хуже. Это из-за дырок, которые описаны в следующем ответе.\n",
    "\n",
    "**Q:** Почему перплексия униграмной модели меньше, чем триграмной?  \n",
    "**A:** В триграмной модели появляются дырки, т.е. не присутствующие в словаре последовательности токенов, частота которых 0. Из-за этого резко уменьшается перплексия.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add-one smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простейший вид сглаживания — **сглаживание Лапласа**. Чтобы избавиться от нулевых вероятностей $P(w_{N} \\mid w_1^{N - 1})$, будем использовать формулу:\n",
    "\n",
    "$$\n",
    "    \\hat P_{AOS}(w_{N} \\mid w_1^{N - 1}) = \\frac{c(w_1^N) + \\delta}{c(w_1^{N-1}) + \\delta V},\n",
    "$$\n",
    "\n",
    "где $V$ — это размер словаря, а $\\delta$ — некоторая фиксированная константа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте класс, осуществляющий сглаживание Лапласа. Он должен иметь аналогичный интерфейс, как и StraightforwardProbabilityEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LaplaceProbabilityEstimator:\n",
    "    \"\"\"Class for probability estimations of type P(word | context).\n",
    "    \n",
    "    P(word | context) = (c(context + word) + delta) / (c(context) + delta * V), where\n",
    "    c(sequence) - number of occurances of the sequence in the corpus,\n",
    "    delta - some constant,\n",
    "    V - number of different words in corpus.\n",
    "    \n",
    "    Args:\n",
    "        storage(NGramStorage): Object of NGramStorage class which will\n",
    "            be used to extract frequencies of ngrams.\n",
    "        delta(float): Smoothing parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage, delta=1.):\n",
    "        self.__storage = storage\n",
    "        self.__delta = delta\n",
    "        \n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) up to the length of max_n.\n",
    "        \"\"\"\n",
    "        if len(context) + 1 > self.__storage.max_n:\n",
    "            return context[-self.__storage.max_n + 1:]\n",
    "        return context\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(word, str):\n",
    "            raise TypeError('word must be a string!')\n",
    "        if not isinstance(context, tuple):\n",
    "            raise TypeError('context must be a tuple!')\n",
    "            \n",
    "        ### YOUR CODE HERE\n",
    "        # If context is too large, let's cut it.\n",
    "        context = self.cut_context(context)\n",
    "        phrase_counts = self.__storage(context + (word, ))\n",
    "        context_counts = self.__storage(context)\n",
    "        # Avoiding 0 / 0.\n",
    "        if context_counts == 0:\n",
    "            return 0.\n",
    "        prob = (phrase_counts + self.__delta) / (context_counts + self.__delta * self.__storage[0][tuple()])\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберите наилучший параметр $\\delta$ для данного корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace estimator perplexity = 224.21185662915858\n",
      "1.37448481923e-05\n"
     ]
    }
   ],
   "source": [
    "# Try to find out best delta parameter. We will not provide you any strater code.\n",
    "### YOUR CODE HERE\n",
    "def get_perplexity(delta):\n",
    "    return perplexity(LaplaceProbabilityEstimator(storage, delta), test_sents)\n",
    "\n",
    "deltas = np.linspace(0, 0.00005, 100)\n",
    "perplexities = np.array([get_perplexity(delta) for delta in deltas])\n",
    "best_delta = deltas[np.argmin(perplexities)]\n",
    "### END YOUR CODE\n",
    "\n",
    "# Initialize estimator\n",
    "laplace_estimator = LaplaceProbabilityEstimator(storage, best_delta)\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Laplace estimator perplexity = {}'.format(perplexity(laplace_estimator, test_sents)))\n",
    "print(laplace_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fd18efcb2b0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHmhJREFUeJzt3X+UHWWd5/H359btNJjEAaEZIT8IcFAWZiEZW0RZf+DM\nrMoojMqexVHUGeZEHFQ4wnEWcB2PHnZ3RoVdZVZOPHgc5+ACSmTBgz9gjQ7MLGASE0JoccMPh2AW\nA6wk/Er63v7uH1W3U925t24l6ep0pz6vQ5+u+9yn6j5PEp7vfer7VJUiAjMzs14a+7sBZmY2szlQ\nmJlZIQcKMzMr5EBhZmaFHCjMzKyQA4WZmRWqLFBIWiRplaQRSRslXZR77+OSHsrK/zZXfpmkTdl7\nb6uqbWZmVl6zwmO3gEsiYq2k+cAaSXcAvwucDZwcETskHQEg6UTgXOAk4CjgTkmvioh2hW00M7M+\nKptRRMSWiFibbW8HRoAFwEeB/xIRO7L3fpPtcjZwQ0TsiIhHgU3AqVW1z8zMyqlyRjFO0hJgGXAv\n8AXgjZKuBF4CLo2In5EGkXtyu23OyiYfazmwHGDu3LmvOeGEEyptu5nZgWbNmjVPRcRQ2fqVBwpJ\n84CbgYsjYpukJnAocBrwWuAmSccC6rL7bvcXiYgVwAqA4eHhWL16dWVtNzM7EEn61Z7Ur3TVk6QB\n0iBxfUSszIo3AysjdR8wBhyelS/K7b4Q+HWV7TMzs/6qXPUk4DpgJCKuyr11C/DWrM6rgDnAU8Ct\nwLmSBiUdAxwP3FdV+8zMrJwqTz2dDpwHbJC0Liu7HPg68HVJDwA7gQ9FegvbjZJuAh4kXTF1oVc8\nmZntf5UFioi4m+55B4AP9NjnSuDKqtpkZmZ7zldmm5lZIQcKMzMr5EBhZmaFahkotjz7Ilf96CEe\n2frc/m6KmdmMV8tAsXX7Dr784008+tTz+7spZmYzXi0DRdJIF2O1xna78NvMzCapZaBoNtJutx0o\nzMz6qmWg8IzCzKy8WgaKZhYo2mNj+7klZmYzXy0DxfiMou0ZhZlZP7UMFM3Ep57MzMqqZaBwjsLM\nrLxaBoqBzqqntnMUZmb91DJQJD71ZGZWWi0Dxa5VTw4UZmb91DJQOEdhZlZeLQOFr8w2MyuvloEi\nm1DQcjLbzKyvWgYKSTQb8qknM7MSahkoIL3ozqeezMz6qyxQSFokaZWkEUkbJV2UlX9W0hOS1mU/\nZ2blSyS9mCu/tqq2QZqn8IzCzKy/ZoXHbgGXRMRaSfOBNZLuyN67OiK+2GWfhyNiaYVtGpc0PKMw\nMyujskAREVuALdn2dkkjwIKqPm9PpTkKJ7PNzPqZlhyFpCXAMuDerOhjku6X9HVJh+aqHiPp55J+\nKumNPY61XNJqSau3bt26123yjMLMrJzKA4WkecDNwMURsQ34KnAcsJR0xvGlrOoWYHFELAM+CXxL\n0ssnHy8iVkTEcEQMDw0N7XW7mg35NuNmZiVUGigkDZAGiesjYiVARDwZEe2IGAO+Bpyale+IiKez\n7TXAw8Crqmpbknh5rJlZGVWuehJwHTASEVflyo/MVXs38EBWPiQpybaPBY4HHqmqfV71ZGZWTpWr\nnk4HzgM2SFqXlV0OvE/SUiCAx4CPZO+9CficpBbQBi6IiGeqalyzIT8K1cyshCpXPd0NqMtbt/eo\nfzPpaappkThHYWZWiq/MNjOzQrUNFIlzFGZmpdQ2UDR9HYWZWSm1DRSJr8w2MyultoHCF9yZmZVT\n20CR+HkUZmal1DZQDCQN5yjMzEqobaDwjMLMrJzaBgpfmW1mVk5tA4VnFGZm5dQ2UPg6CjOzcmob\nKJJGw8tjzcxKqG2g8KNQzczKqW+g8E0BzcxKqW+gcDLbzKyU2gaKpNGg7RyFmVlftQ0UTT8z28ys\nlNoGisTLY83MSqltoPCqJzOzcmobKJKGGAsY86zCzKxQZYFC0iJJqySNSNoo6aKs/LOSnpC0Lvs5\nM7fPZZI2SXpI0tuqahukMwrAeQozsz6aFR67BVwSEWslzQfWSLoje+/qiPhivrKkE4FzgZOAo4A7\nJb0qItpVNK6ZpDHSeQozs2KVzSgiYktErM22twMjwIKCXc4GboiIHRHxKLAJOLWq9u2aUThPYWZW\nZFpyFJKWAMuAe7Oij0m6X9LXJR2alS0AHs/ttpkugUXSckmrJa3eunXrXrcpyQKFZxRmZsUqDxSS\n5gE3AxdHxDbgq8BxwFJgC/ClTtUuu+82ikfEiogYjojhoaGhvW6XcxRmZuVUGigkDZAGiesjYiVA\nRDwZEe2IGAO+xq7TS5uBRbndFwK/rqptScM5CjOzMqpc9STgOmAkIq7KlR+Zq/Zu4IFs+1bgXEmD\nko4Bjgfuq6p9nRnFaNs5CjOzIlWuejodOA/YIGldVnY58D5JS0lPKz0GfAQgIjZKugl4kHTF1IVV\nrXgC5yjMzMqqLFBExN10zzvcXrDPlcCVVbUpr5k4R2FmVkZtr8xuOkdhZlZKbQNF59STH4dqZlas\ntoGi6RyFmVkptQ0USeIrs83MyqhtoPCMwsysnNoGimT8OgoHCjOzIrUNFF71ZGZWTn0DhXMUZmal\n1DdQOEdhZlZKbQNF4rvHmpmVUttA4RyFmVk5tQ0UnlGYmZVT20Ax/uAi32bczKxQbQOFZxRmZuXU\nNlB0lsc6R2FmVqy+gSJLZntGYWZWrMaBIptROEdhZlaotoEi8RPuzMxKqW2g8JXZZmbl1DZQeNWT\nmVk5lQUKSYskrZI0ImmjpIsmvX+ppJB0ePb6LZKelbQu+/lMVW2DXDLbtxk3MyvUrPDYLeCSiFgr\naT6wRtIdEfGgpEXAHwH/MmmfuyLinRW2aVw2oaDtu8eamRWqbEYREVsiYm22vR0YARZkb18NfArY\nb1/nJTGQyKeezMz6mJYchaQlwDLgXklnAU9ExPouVV8vab2k70s6qcexlktaLWn11q1b96ldSUNO\nZpuZ9VF5oJA0D7gZuJj0dNQVQLf8w1rg6Ig4BfgKcEu340XEiogYjojhoaGhfWpbs9HwjMLMrI9K\nA4WkAdIgcX1ErASOA44B1kt6DFgIrJX0yojYFhHPAUTE7cBAJ9FdFc8ozMz6K5XMlvSKiHhmTw4s\nScB1wEhEXAUQERuAI3J1HgOGI+IpSa8EnoyIkHQqaRB7ek8+c081G/KjUM3M+ig7o7hX0rclnZkF\ngDJOB84D3ppb8npmQf1zgAckrQe+DJwbEZV+3U8a8vJYM7M+yi6PfRXwh8CfA1+RdCPwjYj4Za8d\nIuJuoDCoRMSS3PY1wDUl2zMl0hmFA4WZWZFSM4pI3RER7wP+AvgQcJ+kn0p6faUtrFCSOEdhZtZP\n2RzFYcAHSE8lPQl8HLgVWAp8mzRBPesMeNWTmVlfZU89/W/gH4A/iYjNufLVkq6d+mZNj3TVk5PZ\nZmZFygaKUyLixXyBpMMj4qmI+JsK2jUtnMw2M+tvT1Y9ndZ5Iem9wD9X06Tp03SOwsysr7IzivcD\nX5f0E+Ao4DDgrVU1arokzlGYmfVVKlBExAZJV5LmKbYDb5qUq5iVfMGdmVl/ZVc9XUd6+42TSa+p\nuE3SNRHxd1U2rmrOUZiZ9Vc2R/EAcEZEPBoRPwROA36/umZNj6bv9WRm1lfZC+6uBg6S9Ors9bMR\ncX6lLZsGzcQ5CjOzfkoFCknvAtYBP8heL5V0a5UNmw6eUZiZ9Vf21NNngVOB3wJExDpm6dXYeYnv\n9WRm1lfZQNGKiGcnlc36EbbpK7PNzPoqex3FA5L+FEgkHQ98ggPggjvPKMzM+is7o/g4cBKwA/gf\nwDbSR5vOak0vjzUz66vsBXcvkD7r+opqmzO9kkbDyWwzsz4KA4Wk2yjIRUTEWVPeomk0kPjKbDOz\nfvrNKL44La3YTxIvjzUz66swUETETzvbkuYAJ5DOMB6KiJ0Vt61yfhSqmVl/Ze/19MfAtcDDpM/B\nPkbSRyLi+1U2rmpJo0HbyWwzs0JlVz19ifReT2+JiDcDZwBXF+0gaZGkVZJGJG2UdNGk9y+VFJIO\nz15L0pclbZJ0v6TK7yXVTDyjMDPrp+x1FL+JiE25148Av+mzTwu4JCLWSpoPrJF0R0Q8KGkR8EfA\nv+TqvwM4Pvt5HfDV7HdlEt9m3Mysr7Izio2Sbpf0YUkfAm4DfibpPZLe022HiNgSEWuz7e3ACLAg\ne/tq4FNMXFF1NvDNSN0DHCLpyL3oU2nOUZiZ9Vd2RnEQ8CTw5uz1VuAVwLtIB/uVRTtLWgIsI32k\n6lnAExGxXlK+2gLg8dzrzVnZlknHWg4sB1i8eHHJ5neXNEQEjI0FjYb672BmVkN9A4WkBLg/u9X4\nHpM0D7iZ9EruFulFe/+2W9UuZbt93Y+IFcAKgOHh4X2aDgwk6YSqNRbMcaAwM+uq76mniGgDe3Vh\nnaQB0iBxfUSsJH1K3jHAekmPAQuBtZJeSTqDWJTbfSHw67353LKSLDj4Wgozs97Knnr6Z0nXADcC\nz3cKOzmIbpSeV7oOGImIq7L6G4AjcnUeA4Yj4qns+RYfk3QDaRL72YjYsvuRp04zCxRpQjup8qPM\nzGatsoHiDdnvz+XKAnhrwT6nA+cBGySty8ouj4jbe9S/HTgT2AS8APxZybbtNc8ozMz6K3tTwDP2\n9MARcTfd8w75Okty2wFcuKefsy92zSgcKMzMein7KNTflXSdpO9nr0+UNOufmZ00smS2r842M+up\n7HUU3wB+CByVvf4lB8jzKABfdGdmVqBsoDg8Im4CxgAiogW0K2vVNGkmzlGYmfVTNlA8L+kwsusa\nJJ0GTH6G9qyTOEdhZtZX2VVPnwRuBY6V9E/AEHBOZa2aJs0sR+EZhZlZb2UDxYPAd0mXrW4HbiHN\nU8xq4zMKJ7PNzHoqe+rpm6QPLfpPwFdI7/D6D1U1aro0fR2FmVlfZWcUr46IU3KvV0laX0WDplOS\nJbNHverJzKynsjOKn2cJbAAkvQ74p2qaNH08ozAz66/sjOJ1wAcldR40tBgYkbSB9KLqkytpXcWc\nozAz669soHh7pa3YTzq3GfeMwsyst7L3evpV1Q3ZHxJfmW1m1lfZHMUByTkKM7P+ah0ofGW2mVl/\ntQ4UvjLbzKy/WgeKzoxitO0chZlZL7UOFM5RmJn1V+tA4RyFmVl/tQ4Uvo7CzKy/WgcKzyjMzPqr\nLFBIWiRplaQRSRslXZSVf17S/ZLWSfqRpKOy8rdIejYrXyfpM1W1rWM8R+FktplZT2Vv4bE3WsAl\nEbFW0nxgjaQ7gC9ExH8EkPQJ4DPABdk+d0XEOyts0wSdu8d6RmFm1ltlM4qI2BIRa7Pt7cAIsCAi\ntuWqzSV7vOr+0PSpJzOzvqYlRyFpCbAMuDd7faWkx4H3k84oOl4vab2k70s6qcexlktaLWn11q1b\n96ldiZfHmpn1VXmgkDQPuBm4uDObiIgrImIRcD3wsazqWuDo7AFJXyF93OpuImJFRAxHxPDQ0NA+\nta1zZbZvM25m1lulgULSAGmQuD4iVnap8i3gvQARsS0insu2bwcGJB1eZfuShpCg7bvHmpn1VOWq\nJwHXASMRcVWu/PhctbOAX2Tlr8z2QdKpWduerqp9Hc2GnKMwMytQ5aqn04HzgA2S1mVllwPnS3o1\nMAb8il0rns4BPiqpBbwInBsRlY/gSUPOUZiZFagsUETE3YC6vHV7j/rXANdU1Z5emo2GZxRmZgVq\nfWU2eEZhZtZP7QNFsyHfZtzMrEDtA4VnFGZmxWofKLzqycysmANF0vCMwsysgAOFZxRmZoVqHyjS\nHIWT2WZmvThQNOR7PZmZFah9oGgmPvVkZlak9oEi8ZXZZmaFah8oms5RmJkVqn2gcI7CzKxY7QPF\nQOIrs83MitQ+UDhHYWZWrPaBoul7PZmZFap9oEh8ZbaZWaHaB4pmQ7R8m3Ezs55qHyh8m3Ezs2K1\nDxS+KaCZWTEHCt9m3MysUGWBQtIiSaskjUjaKOmirPzzku6XtE7SjyQdlZVL0pclbcre//2q2paX\nziicozAz66XKGUULuCQi/hVwGnChpBOBL0TEyRGxFPge8Jms/juA47Of5cBXK2zbOOcozMyKVRYo\nImJLRKzNtrcDI8CCiNiWqzYX6IzSZwPfjNQ9wCGSjqyqfR3OUZiZFWtOx4dIWgIsA+7NXl8JfBB4\nFjgjq7YAeDy32+asbMukYy0nnXGwePHifW5b0mjQ9r2ezMx6qjyZLWkecDNwcWc2ERFXRMQi4Hrg\nY52qXXbfbQSPiBURMRwRw0NDQ/vcvmYiRp2jMDPrqdJAIWmANEhcHxEru1T5FvDebHszsCj33kLg\n11W2D5yjMDPrp8pVTwKuA0Yi4qpc+fG5amcBv8i2bwU+mK1+Og14NiImnHaqgnMUZmbFqsxRnA6c\nB2yQtC4ruxw4X9KrgTHgV8AF2Xu3A2cCm4AXgD+rsG3jmo0GETA2FjQa3c5+mZnVW2WBIiLupnve\n4fYe9QO4sKr29NJM0ia2xoI5DhRmZrup/ZXZSRYcnKcwM+uu9oGi2ejMKLzyycysm9oHis6Mws/N\nNjPrrvaBYteMwoHCzKyb2geKpJH+EThHYWbWXe0Dxa5VT85RmJl140DhVU9mZoVqHygS5yjMzArV\nPlA0naMwMytU+0Dh5bFmZsVqHyh8wZ2ZWbHaB4okcY7CzKxI7QOFVz2ZmRVzoMiS2c5RmJl150CR\neEZhZlak9oEicTLbzKxQ7QOFcxRmZsVqHyg6M4pR5yjMzLqqfaDwldlmZsVqHyicozAzK1ZZoJC0\nSNIqSSOSNkq6KCv/gqRfSLpf0nclHZKVL5H0oqR12c+1VbUtb7CZ/hGMbNk+HR9nZrbPImJaz4I0\nKzx2C7gkItZKmg+skXQHcAdwWUS0JP0NcBnwV9k+D0fE0grbtJuFhx7MH598JNf+9GGO/J2D+NAb\nlkznx5vZNIoIRttBa2ws/d1Of4+2x2iNpa93tsdoZWWduq12jJd39h1tj43vP7EsGM32abXHGM2O\nm5bHhH1akz57Qvn4+7vXbY8F7zrlKL7yvmXT8udWWaCIiC3Almx7u6QRYEFE/ChX7R7gnKraUIYk\n/uu/X8rO1hh/fetGBpsNzj118f5sktmsMDaWDp7jA2o2yHYGzJ2t3u+NticOtju7bI+20kFx56Tt\n1qTj5Af1zkCafy8/8E/HrXoagmbSYKAhmkmDZkMMJA2aSfY7Kx9INL590ECD5mAzK+tVN/udbZ/w\nyvmV96WjyhnFOElLgGXAvZPe+nPgxtzrYyT9HNgGfDoi7upyrOXAcoDFi6dmQB9IGlzzp8tY/s01\nXPbdDTy89Tk+8ubjOHze4JQc32xPdL71dga7na10kE0H3rR8x/ggvGtA3tkORse3J9afWDbGaCt2\nL2vvGvhb7cgdNzewt8YYzb49V3nqYyA3UM5pNiYMtHNy2wONBnOaDeYONtPXSW5QbjQYaO4aYNPy\ndAAfaO4awHfbLzcw5wfsTt1ux84HhE7e80CiiGojrKR5wE+BKyNiZa78CmAYeE9EhKRBYF5EPC3p\nNcAtwEkRsa3XsYeHh2P16tVT1taXRtt8+pYHWLl2M4PNhPNefzTvf91ijj5s7pR9hs0cnW/EnUF3\nZ2vioNyrfMJ2fqAueL2zHexstdPBuKB+Z7CeahLMyQbZgWbnt8YH2vxgOac5cYCe0xlMs4FxMDdw\nz2mmg/X4dnacOUkyfrzxgTdJ9+18Ix5vQzbgd7abDSEdeIPtTCJpTUQMl65fZaCQNAB8D/hhRFyV\nK/8QcAHwBxHxQo99fwJcGhE9I8FUB4qOh7c+xzU/3sT/XPcEYwHHHj6XM044glOPeQUnHvlyFh56\nsP8h74X2WGQDcHt8IM4PvjvGB+H2pNe5AXrSQD6xTnu3+pM/o/NtfGdrak9DjA/E2SA8p9mYMNDm\ny/MD8sTyxq7yXJ0Jx2g2cuWaMJDPaWYDdHPXAN0Z1A/Eb7m292ZMoFA6kv498ExEXJwrfztwFfDm\niNiaKx/K6rYlHQvcBfzriHim12dUFSg6Hn/mBe4ceZIf/+I33PvIM+Pf9OYf1OTYoXksPORgFhx6\nMEfMH+SweXN4xdxBDn3ZAPMGm8w7qMncOU0OGkim5X/SiMgSYhMTafkE3M7WxNMQnVMJnVML+VMV\n3b41Txhsuwzeo5MG6HRAbo9/S56qUxX5QXmw2WCwmew2EKeDasJgNrgOdhmYd3s9qc7A5P2auwbl\nwUllzaT2K81tFplJgeLfkA72G4DOXPpy4MvAIPB0VnZPRFwg6b3A50hXS7WBv46I24o+o+pAkffi\nzjYj/3cbI1vSn8eeeoEnfvsiT/z2RXa2ik8VDCTioGaSO//ZoNGAhkQiQfofkogIAiAggLFsGdzY\nWNCOoD2WlrWygbeV/VR9vjh/yiI/QA7mvuFOHmy7DeKdb8SDuTq7HXMgmRAIJu/vUxNm+2bGBIrp\nMJ2BopexsWDbS6M8/fxO/t/zO/ntC6M8t6PF9h0tnt/R4qXRNi+Npqdb8t/sxyLGg8CuwBCIXYGj\nIdFQ9ruRBpVGI72aPGmIpNFZASGSxq5VFvnVFLudJ27kB+xdpyo6557z56c7558bPm1hdkDZ00Ax\nLaueDmSNhjjkZXM45GVzYGh/t8bMbOr5xKqZmRVyoDAzs0IOFGZmVsiBwszMCjlQmJlZIQcKMzMr\n5EBhZmaFHCjMzKzQrL4yW9JW4Ff7cIjDgaemqDmzQd36C+5zXbjPe+boiCh9ifCsDhT7StLqPbmM\nfbarW3/Bfa4L97laPvVkZmaFHCjMzKxQ3QPFiv3dgGlWt/6C+1wX7nOFap2jMDOz/uo+ozAzsz4c\nKMzMrNCsCxSS3i7pIUmbJP2HLu8PSroxe/9eSUty712WlT8k6W39jinpmOwY/yc75px+n3EA9/lN\nktZKakk6pwb9/aSkByXdL+l/STq6Bn2+QNIGSesk3S3pxAO9z7n3z5EUkipdbjoT+izpw5K2Zn/P\n6yT9Rd+GR8Ss+QES4GHgWGAOsB44cVKdvwSuzbbPBW7Mtk/M6g8Cx2THSYqOCdwEnJttXwt8tOgz\nDvA+LwFOBr4JnFOD/p4BvCzb/mhN/o5fnvu8s4AfHOh9zl7PB/4RuAcYPtD7DHwYuGZP2j7bZhSn\nApsi4pGI2AncAJw9qc7ZwN9n298B/kCSsvIbImJHRDwKbMqO1/WY2T5vzY5Bdsw/6fMZVZgRfY6I\nxyLifmCson52zJT+roqIF7Lye4CFFfS1Y6b0eVvu8+YCVa50mRF9znwe+Fvgpanu5CQzqc97ZLYF\nigXA47nXm7OyrnUiogU8CxxWsG+v8sOA32bHmPxZvT6jCjOlz9NlJvb3fOD7e9GXsmZMnyVdKOlh\n0oHzE/vUq2Izos+SlgGLIuJ7+96lvmZEnzPvzU6rfkfSon4Nn22Botu39snfenrVmarysu2YKjOl\nz9NlRvVX0geAYeALXepOlRnT54j4u4g4Dvgr4NNdWzs19nufJTWAq4FLCto5lfZ7n7PftwFLIuJk\n4E52zWB6mm2BYjOQj34LgV/3qiOpCfwO8EzBvr3KnwIOyY4x+bN6fUYVZkqfp8uM6a+kPwSuAM6K\niB371KtiM6bPOTewD6cqSpgJfZ4P/B7wE0mPAacBt1aY0J4JfSYins79e/4a8Jq+La8qcVNRMqgJ\nPEKazOkkbk6aVOdCJiaDbsq2T2JiMugR0kRQz2MC32ZiMugviz7jQO5z7rO+QbXJ7BnRX2AZaZLw\n+Br9uz4+93nvAlYf6H2e9Hk/odpk9ozoM3Bk7vPeDdzTt+1V/09QwR/2mcAvs/+Jr8jKPkf6rQ/g\noOwPaBNwH3Bsbt8rsv0eAt5RdMys/NjsGJuyYw72+4wDuM+vJf328jzwNLDxAO/vncCTwLrs59Ya\n/B3/N2Bj1t9VTBrEDsQ+T2rPT6gwUMyUPgP/Oft7Xp/9PZ/Qr92+hYeZmRWabTkKMzObZg4UZmZW\nyIHCzMwKOVCYmVkhBwozMyvkQGG2lyR9VtKlZd7P7th51PS1zmzqOFCYTY8PAw4UNis5UJjtAUlX\nZPf+vxN4dVZ2nKQfSFoj6S5JJ0za5xzS+0Vdn93//2BJn5H0M0kPSFpR4d2HzfaZA4VZSZJeQ3pb\nhWXAe0ivVof0Ifcfj4jXAJcC/z2/X0R8B1gNvD8ilkbEi6TPA3htRPwecDDwzmnqhtkea/avYmaZ\nNwLfjew5FZJuJb3lwhuAb+cmBYMljnWGpE8BLwNeQXpLhdumvMVmU8CBwmzPTL7nTYP0vv9Lyx5A\n0kGks47hiHhc0mdJA47ZjORTT2bl/SPw7izHMJ/0DqsvAI9K+ncASp3SZd/tpLe1hl1B4SlJ84BK\nn0Futq8cKMxKioi1wI2kd1e9Gbgre+v9wPmS1pOeQpr8eEtIb89+raR1wA7S5wBsAG4BflZty832\nje8ea2ZmhTyjMDOzQg4UZmZWyIHCzMwKOVCYmVkhBwozMyvkQGFmZoUcKMzMrND/B2yEd39CrB0D\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd18ef483c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Рисуем, чтобы удостовериться, что выбран правильный интервал поиска дельты\n",
    "\n",
    "axis = plot.gca()\n",
    "plot.plot(deltas, perplexities)\n",
    "axis.set_xlabel('delta')\n",
    "axis.set_ylabel('perplexy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stupid backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **простого отката** довольно понятна. Если у нас есть достаточно информцаии для подсчета вероятности $k$-грам, то будем использовать $k$-грамы. Иначе будем использовать вероятности $(k-1)$-грам с некоторым множителем, например, $0.4$, и так далее. К сожалению, в данном случае мы получим не вероятностное распределение, но в большинстве задач это не имеет принципиального значения. Если это все же важно, то необходимо подобрать множитель соответствующим образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте класс, симулирующий сглаживание простым откатом. Он должен иметь аналогичный интерфейс, как и StraightforwardProbabilityEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StupidBackoffProbabilityEstimator:\n",
    "    \"\"\"Class for stupid backoff probability estimations.\n",
    "    \n",
    "    P(word | context) =\n",
    "        P'(word | context),                  if  P'(word | context) > 0;\n",
    "        P'(word | context[1:]) * multiplier, if  P'(word | context) == 0\n",
    "                                             and P'(word | context[1:]) > 0;\n",
    "        ...\n",
    "    P'(word | context) - probability of a word provided context of a base estimator.\n",
    "    \n",
    "    Args:\n",
    "        base_estimator(BaseProbabilityEstimator): Object of BaseProbabilityEstimator\n",
    "            or some other class which can estimate conditional probabilities.\n",
    "        multiplier (float): Multiplier which is used for probability estimations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_estimator, multiplier=0.1):\n",
    "        self.__base_estimator = base_estimator\n",
    "        self.__mult = multiplier\n",
    "\n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) by one word.\n",
    "        \"\"\"\n",
    "        if len(context) != 1:\n",
    "            return context[1:]\n",
    "        else:\n",
    "            return tuple()\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        counter = 0\n",
    "        while self.__base_estimator(word, context) == 0:\n",
    "            context = self.cut_context(context)\n",
    "            counter += 1\n",
    "        return (self.__mult ** counter) * self.__base_estimator(word, context)\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stupid backoff estimator perplexity = 120.52657075115137\n",
      "1.37448481923e-05\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "sbackoff_estimator = StupidBackoffProbabilityEstimator(simple_estimator, .4)\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Stupid backoff estimator perplexity = {}'.format(perplexity(sbackoff_estimator, test_sents)))\n",
    "print(laplace_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на следующие вопросы (внутри ipython ноутбука):\n",
    "\n",
    "**Q:** Почему бессмысленно измерять перплексию в случае **Stupid backoff**?  \n",
    "**A:** Из-за тупого отнятия вероятности распределение перестает быть вероятностным (сумма всех вероятностей не единица). Перплексия, как вероятностная величина теряет значение.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае идея сглаживания посредством **интерполяции** также крайне проста. Пусть у нас есть $N$-грамная модель. Заведем вектор $\\bar\\lambda = (\\lambda_1, \\dots, \\lambda_N)$, такой, что $\\sum_i\\lambda_i = 1$ и $\\lambda_i \\geq 0$. Тогда\n",
    "\n",
    "$$\n",
    "    \\hat P_{IS}(w_{N} \\mid w_1^{N-1}) = \\sum_{i=1}^N \\lambda_i \\hat P_{S}(w_N \\mid w_{N-i+1}^{N-1}).\n",
    "$$\n",
    "\n",
    "Придумайте, как обойтись одним вектором $\\bar\\lambda$, т.е. пользоваться им как в случае контекста длины $N$, так и при контексте меньшей длины (например, в начале предложения). Если мы просто обрубим сумму, то у нас уже не будет вероятностное распределение, что, конечно же, плохо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InterpolationProbabilityEstimator:\n",
    "    \"\"\"Class for interpolation probability estimations.\n",
    "    \n",
    "    P(word | context) =\n",
    "        lambda_N * P'(word | context) +\n",
    "        lambda_{N-1} * P'(word | context[1:]) +\n",
    "        ... +\n",
    "        lambda_1 * P'(word)\n",
    "    P'(word | context) - probability of a word provided context of a base estimator.\n",
    "    \n",
    "    Args:\n",
    "        base_estimator(BaseProbabilityEstimator): Object of BaseProbabilityEstimator\n",
    "            or some other class which can estimate conditional probabilities.\n",
    "        lambdas (np.array[float]): Lambdas which are used for probability estimations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_estimator, lambdas):\n",
    "        self.lambdas = lambdas\n",
    "        self.__base_estimator = base_estimator\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        def normalized(lambdas):\n",
    "            lambdas_sum = sum(lambdas)\n",
    "            return np.array([one_lambda / lambdas_sum for one_lambda in lambdas])\n",
    "        current_lambdas = normalized(self.lambdas[:len(context)])\n",
    "        return np.sum([\n",
    "            one_lambda * self.__base_estimator(word, context[i:])\n",
    "            for one_lambda, i in zip(reversed(current_lambdas), range(len(context)))\n",
    "        ])\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation estimator perplexity = 302.91205717785704\n",
      "1.37448481923e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize estimator\n",
    "interpol_estimator = InterpolationProbabilityEstimator(simple_estimator, np.array([0.2, 0.2, 0.6]))\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Interpolation estimator perplexity = {}'.format(perplexity(interpol_estimator, test_sents)))\n",
    "print(laplace_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучить значения параметров $\\lambda$ можно с помощью EM-алгоритма, но мы не будем этого здесь делать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kneser-Ney smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея данного сглаживания заключается в том, что словам, которые участвуют в большом количестве контекстов, присваиваются большие вероятности, а те, которые используются в паре-тройке контекстов, получают маленькие вероятности. Формулы приведены на слайде 37 лекции.\n",
    "Реализуйте данный подход."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KneserNeyProbabilityEstimator:\n",
    "    \"\"\"Class for probability estimations of type P(word | context).\n",
    "    \n",
    "    P(word | context) = ...\n",
    "    \n",
    "    Args:\n",
    "        storage(NGramStorage): Object of NGramStorage class which will\n",
    "            be used to extract frequencies of ngrams.\n",
    "        delta(float): KneserNey parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage, delta=1.):\n",
    "        self.__storage = storage\n",
    "        self.__delta = delta\n",
    "        \n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) up to the length of max_n.\n",
    "        \"\"\"\n",
    "        if len(context) + 1 > self.__storage.max_n:\n",
    "            context = context[-self.__storage.max_n + 1:]\n",
    "        return context\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(word, str):\n",
    "            raise TypeError('word must be a string!')\n",
    "        if not isinstance(context, tuple):\n",
    "            raise TypeError('word must be a string!')\n",
    "        # If context is too large, let's cut it.\n",
    "        context = self.cut_context(context)\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        prob = 1.\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize estimator\n",
    "kn_estimator = KneserNeyProbabilityEstimator(storage)\n",
    "\n",
    "# Estimating perplexity\n",
    "print('Simple estimator perplexity = {}'.format(perplexity(kn_estimator, test_sents)))\n",
    "print(simple_estimator.prob('To be'.split()))\n",
    "print(simple_estimator.prob('To be or not to be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определение языка документа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Постановка задачи:**  \n",
    "Одна из задач, которая может быть решена при помощи языковых моделей $-$ **определение языка документа**. Реализуйте два классификатора для определения языка документа:\n",
    "1. Наивный классификатор, который будет учитывать частоты символов и выбирать язык текста по признаку: распределение частот символов \"наиболее похоже\" на распределение частот символов в выбранном языке.\n",
    "2. Классификатор на основе языковых моделей. Сами придумайте, как он должен работать.  \n",
    "_Подсказка_: лучше считать n-грамы не по словам, а по символам.\n",
    "\n",
    "---\n",
    "\n",
    "**Как представлены данные:**  \n",
    "Во всех текстовых файлах на каждой строчке записано отдельное предложение.\n",
    "1. В папке _data_ находятся две папки: _full_ и _plain_. В _full_ находятся тексты в той форме, что они были взяты из сети, в _plain_ находятся те же самые тексты, но с них сначала была снята диакритика, а затем русский и греческий тексты были транслитерованы в английский.\n",
    "2. В каждой из папок _full_ и _plain_ находятся папки _train_ и _test_.\n",
    "3. В _train_ находятся файлы с текстами с говорящими именами, например, _ru.txt_, _en.txt_.\n",
    "4. В _test_ находятся файлы _1.txt_, _2.txt_, $\\dots$ в которых хранятся тексты, язык которых нужно определить. В этой же папке находится файл _ans.csv_, в котором вы можете найти правильные ответы и проверить, насколько хорошо сработали Ваши алгоритмы.\n",
    "\n",
    "---\n",
    "\n",
    "**Что нужно сделать:**  \n",
    "Напишите два своих классификатора (которые описаны в постановке задачи) и получите максимально возможное accuracy на test-сете. Разрешается использовать только _train_ для обучения.\n",
    "\n",
    "---\n",
    "\n",
    "**В данном задании мы не предоставляем стартового кода!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the code and estimate accuracy of your method.\n",
    "# Create your own classifiers.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
