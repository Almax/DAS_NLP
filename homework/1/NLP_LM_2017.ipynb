{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание №1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тема: Языковое моделирование и определение языка.\n",
    "\n",
    "\n",
    "**Выдана**:   14 сентября 2017\n",
    "\n",
    "**Дедлайн**:   <font color='red'>9:00 утра 28 сентября 2017</font>\n",
    "\n",
    "**Среда выполнения**: Jupyter Notebook (Python 3)\n",
    "\n",
    "#### Правила:\n",
    "\n",
    "Результат выполнения задания $-$ отчет в формате Jupyter Notebook с кодом и выводами. В ходе выполнения задания требуется реализовать все необходимые алгоритмы, провести эксперименты и ответить на поставленные вопросы. Дополнительные выводы приветствуются. Чем меньше кода и больше комментариев $-$ тем лучше.\n",
    "\n",
    "Все ячейки должны быть \"выполненными\", при этом результат должен воспроизвдиться при проверке (на Python 3). Если какой-то код не был запущен или отрабатывает с ошибками, то пункт не засчитывается. Задание, сданное после дедлайна, _не принимается_. Совсем.\n",
    "\n",
    "\n",
    "Задание выполняется самостоятельно. Вы можете обсуждать идеи, объяснять друг другу материал, но не можете обмениваться частями своего кода. Если какие-то студенты будут уличены в списывании, все они автоматически получат за эту работу 0 баллов, а также предвзято негативное отношение семинаристов в будущем. Если вы нашли в Интернете какой-то код, который собираетесь заимствовать, обязательно укажите это в задании: вполне вероятно, что вы не единственный, кто найдёт и использует эту информацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Постановка задачи:\n",
    "\n",
    "В данной лабораторной работе Вам предстоит реализовать n-грамную языковую модель с несколькими видами сглаживания:\n",
    "- Add-one smoothing\n",
    "- Stupid backoff\n",
    "- Interpolation smoothing\n",
    "- Kneser-Ney smoothing\n",
    "\n",
    "Вы обучите ее на готовых корпусах, оцените качество и проведете ряд экспериментов. Во второй части задания Вы примените реализованную модель (но с буквенными n-граммами) к задаче распознавания языка. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель языкового моделирования заключается в том, чтобы присвоить некоторые вероятности предложениям. Задача состоит в подсчете вероятности $P(W) = P(w_1, \\dots, w_n)$ или $P(w_n \\mid w_1, \\dots, w_{n-1})$. Модель, умеющая вычислять хотя бы одну из этих двух вероятностей, называется **языковой моделью** (LM от Language Model).\n",
    "\n",
    "Согласно **цепному правилу** (chain rule):\n",
    "\n",
    "$$P(X_1, \\dots, X_n) = P(X_1)P(X_2 \\mid X_1)\\dots P(X_n \\mid X_1, \\dots, X_{n-1}).$$ \n",
    "\n",
    "Также мы знаем, что\n",
    "\n",
    "$$\n",
    "    P(X_n \\mid X_1, \\dots, X_{n-1}) = \\frac{P(X_1, \\dots, X_n)}{P(X_1, \\dots, X_{n-1})},\n",
    "$$\n",
    "\n",
    "следовательно, для того чтобы оценить $P(X_n \\mid X_1, \\dots, X_{n-1})$ нужно посчитать $P(X_1, \\dots, X_n)$ и $P(X_1, \\dots, X_{n-1})$. Но эти вероятности будут чрезвычайно малы, если мы возьмем большое $n$, так множество предложений из $n$ слов растет экспоненциально. Для упрощения применим **марковское предположение**: \n",
    "\n",
    "$$P(X_n \\mid X_1, \\dots, X_{n-1}) = P(X_n \\mid X_{n - k + 1}, \\dots, X_{n-1})$$\n",
    "\n",
    "для некоторого фиксированного (небольшого) $k$. Это предположение говорит о том, что $X_{n}$ не зависит от $X_{1}, \\dots, X_{n - k}$, то есть на следующее слово влияет лишь контекст из предыдущих $k - 1$ слова. Таким образом, мы получаем финальную вероятность:\n",
    "\n",
    "$$\n",
    "    P(w_1, \\dots, w_n) = \\prod_i P(w_i \\mid w_{i-k+1}, \\dots, w_{i - 1}).\n",
    "$$\n",
    "\n",
    "Далее для краткости будем обозначать $w_{i-k}^i := w_{i-k}, \\dots, w_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Хранилище n-грам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала выполним вспомогательную работу. Следуйте комментариям, чтобы написать NGramStorage с удобным интерфейсом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramStorage:\n",
    "    \"\"\"Storage for ngrams' frequencies.\n",
    "\n",
    "    Args:\n",
    "        sents (list[list[str]]): List of sentences from which ngram\n",
    "            frequencies are extracted.\n",
    "        max_n (int): Upper bound of the length of ngrams.\n",
    "            For instance if max_n = 2, then storage will store\n",
    "            0, 1, 2-grams.\n",
    "\n",
    "    Attributes:\n",
    "        max_n (Readonly(int)): Upper bound of the length of ngrams.\n",
    "    \"\"\"\n",
    "\n",
    "    def __make_lower(self, n_gram):\n",
    "        return tuple([word.lower() for word in n_gram])\n",
    "\n",
    "    def __init__(self, sents=[], max_n=0):\n",
    "        self.__max_n = max_n\n",
    "        self.__ngrams = {i: Counter() for i in range(self.__max_n + 1)}\n",
    "        # self._ngrams[K] should have the following interface:\n",
    "        # self._ngrams[K][(w_1, ..., w_K)] = number of times w_1, ..., w_K occured in words\n",
    "        # self._ngrams[0][()] = number of all words\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        def handle_sentence(sentence):\n",
    "            for length in range(1, max_n + 1):\n",
    "                for i in range(len(sentence)):\n",
    "                    new_n_gram = tuple(sentence[i:(i + length)])\n",
    "                    if len(new_n_gram) == length:\n",
    "                        self.__ngrams[length][new_n_gram] += 1\n",
    "                        if length == 1:\n",
    "                            self.__ngrams[0][tuple()] += 1\n",
    "        for sentence in sents:\n",
    "            handle_sentence(self.__make_lower(sentence))\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "    def add_unk_token(self):\n",
    "        \"\"\"Add UNK token to 1-grams.\"\"\"\n",
    "        # In order to avoid zero probabilites \n",
    "        if self.__max_n == 0 or 'UNK' in self.__ngrams[1]:\n",
    "            return\n",
    "        self.__ngrams[0][()] += 1\n",
    "        self.__ngrams[1][('UNK', )] = 1\n",
    "        \n",
    "    @property\n",
    "    def max_n(self):\n",
    "        \"\"\"Get max_n\"\"\"\n",
    "        return self.__max_n\n",
    "        \n",
    "    def __getitem__(self, k):\n",
    "        \"\"\"Get dictionary of k-gram frequencies.\n",
    "        \n",
    "        Args:\n",
    "            k (int): length of returning ngrams' frequencies.\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary (in fact Counter) of k-gram frequencies.\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(k, int):\n",
    "            raise TypeError('k (length of ngrams) must be an integer!')\n",
    "        if k > self.__max_n:\n",
    "            raise ValueError('k (length of ngrams) must be less or equal to the maximal length!')\n",
    "        return self.__ngrams[k]\n",
    "    \n",
    "    def __call__(self, ngram):\n",
    "        \"\"\"Return frequency of a given ngram.\n",
    "        \n",
    "        Args:\n",
    "            ngram (tuple): ngram for which frequency should be computed.\n",
    "            \n",
    "        Returns:\n",
    "            Frequency (int) of a given ngram.\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(ngram, tuple):\n",
    "            raise TypeError('ngram must be a tuple!')\n",
    "        if len(ngram) > self.__max_n:\n",
    "            raise ValueError('length of ngram must be less or equal to the maximal length!')\n",
    "        if len(ngram) == 1 and ngram not in self.__ngrams[1]:\n",
    "            return self.__ngrams[1][('UNK', )]\n",
    "        return self.__ngrams[len(ngram)][self.__make_lower(ngram)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте brown корпус, обучите модель и протестируйте на нескольких примерах последовательностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Uncomment next row and download brown corpus\n",
    "# nltk.download()\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all sentences = 57340\n",
      "Number of train sentences = 45872\n",
      "Number of test sentences = 11468\n"
     ]
    }
   ],
   "source": [
    "all_sents = list(brown.sents())\n",
    "random.shuffle(all_sents)\n",
    "print('Number of all sentences = {}'.format(len(all_sents)))\n",
    "train_sents = all_sents[:int(0.8 * len(all_sents))]\n",
    "test_sents = all_sents[int(0.8 * len(all_sents)):]\n",
    "print('Number of train sentences = {}'.format(len(train_sents)))\n",
    "print('Number of test sentences = {}'.format(len(test_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create storage of 0, 1, 2, 3-grams\n",
    "storage = NGramStorage(train_sents, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372\n",
      "3392\n",
      "24\n",
      "0\n",
      "927300\n"
     ]
    }
   ],
   "source": [
    "# It's time to test your code\n",
    "print(storage(('to', 'be')))\n",
    "print(storage(('or',)))\n",
    "print(storage(('not', 'to', 'be')))\n",
    "print(storage(('somethingweird',)))\n",
    "print(storage(()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для численного измерения качества языковой модели определим **перплексию**:\n",
    "\n",
    "$$\n",
    "    {\\mathbb{P}}(w_1, \\dots, w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_i P(w_i \\mid w_{i - k}, \\dots, w_{i - 1})\\right)^{-\\frac1N},\n",
    "$$\n",
    "\n",
    "Вижно, что минимизация перплексии эквивалентна максимизации правдоподобия модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте функцию по подсчету перплексии. Обратите внимание, что перплексия по корпусу равна произведению вероятностей **всех** предложений в степени $-\\frac1N$, где $N -$ суммарная длина всех предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(estimator, sents):\n",
    "    '''Estimate perplexity of the sequence of words using prob_estimator.'''\n",
    "    ### YOUR CODE HERE\n",
    "    # Avoid log(0) by replacing zero by 10 ** (-50).\n",
    "    log_perp = 0\n",
    "    N = 0\n",
    "    for sentence in sents:\n",
    "        log_perp += np.log(max(estimator.prob(sentence), 10 ** (-100)))\n",
    "        N += len(sentence)\n",
    "    perp = np.exp(-log_perp / N)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Оценка вероятностей n-грам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый и простейший способ оценки вероятностей N-грам следующий:\n",
    "\n",
    "$$\n",
    "    \\hat P_{S}(w_{N} \\mid w_1^{N - 1}) = \\frac{c(w_1^N)}{c(w_1^{N-1})}.\n",
    "$$\n",
    "\n",
    "где $c(w_1^N)$ — это число последовательностей $w_1, \\dots, w_N$ в корпусе, $S$ символизирует Straightforward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StraightforwardProbabilityEstimator:\n",
    "    \"\"\"Class for simplest probability estimations of type P(word | context).\n",
    "    \n",
    "    P(word | context) = c(context + word) / c(context), where\n",
    "    c(sequence) - number of occurances of the sequence in the corpus.\n",
    "    \n",
    "    Args:\n",
    "        storage(NGramStorage): Object of NGramStorage class which will\n",
    "            be used to extract frequencies of ngrams.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage):\n",
    "        self.__storage = storage\n",
    "        # Adding UNK token to avoid zero probabilities\n",
    "        self.__storage.add_unk_token()\n",
    "        \n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) up to the length of max_n.\n",
    "        \"\"\"\n",
    "        if self.__storage.max_n == 1:\n",
    "            return ()\n",
    "        if len(context) + 1 > self.__storage.max_n:\n",
    "            return context[-self.__storage.max_n + 1:]\n",
    "        return context\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(word, str):\n",
    "            raise TypeError('word must be a string!')\n",
    "        if not isinstance(context, tuple):\n",
    "            raise TypeError('word must be a string!')\n",
    "        # If context is too large, let's cut it.\n",
    "        context = self.cut_context(context)\n",
    "        phrase_counts = self.__storage(context + (word, ))\n",
    "        context_counts = self.__storage(context)\n",
    "        # Avoiding 0 / 0.\n",
    "        if context_counts == 0:\n",
    "            return 0.\n",
    "        return 1. * phrase_counts / context_counts\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple estimator perplexity = 54941.347046562005\n",
      "0.001479562730979477\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "simple_estimator = StraightforwardProbabilityEstimator(storage)\n",
    "\n",
    "# Estimating perplexity\n",
    "print('Simple estimator perplexity = {}'.format(perplexity(simple_estimator, test_sents)))\n",
    "print(simple_estimator.prob('To be'.split()))\n",
    "print(simple_estimator.prob('To be or not to be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем перплексию униграмной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple estimator perplexity = 905.328790440309\n"
     ]
    }
   ],
   "source": [
    "uni_storage = NGramStorage(train_sents, 1)\n",
    "uni_simple_estimator = StraightforwardProbabilityEstimator(uni_storage)\n",
    "print('Simple estimator perplexity = {}'.format(perplexity(uni_simple_estimator, test_sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на следующие вопросы (внутри ipython ноутбука):\n",
    "\n",
    "**Q:** Какие выводы можно сделать? Почему $P(\\text{To be or not to be}) = 0$, хотя мы и добавили UNK токен?  \n",
    "**A:** Вывод: казалось бы, более умная, учитывающая больше признаков модель, а работает хуже. Это из-за дырок, которые описаны в следующем ответе. В триграммной модели они проявляются гораздо чаще, чем в униграммной, т.к. число различных приграмм в языке сильно превышает число различных униграмм.\n",
    "\n",
    "**Q:** Почему перплексия униграмной модели меньше, чем триграмной?  \n",
    "**A:** В триграмной модели появляются дырки, т.е. не присутствующие в словаре последовательности токенов, частота которых 0. Из-за этого резко уменьшается перплексия.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add-one smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простейший вид сглаживания — **сглаживание Лапласа**. Чтобы избавиться от нулевых вероятностей $P(w_{N} \\mid w_1^{N - 1})$, будем использовать формулу:\n",
    "\n",
    "$$\n",
    "    \\hat P_{AOS}(w_{N} \\mid w_1^{N - 1}) = \\frac{c(w_1^N) + \\delta}{c(w_1^{N-1}) + \\delta V},\n",
    "$$\n",
    "\n",
    "где $V$ — это размер словаря, а $\\delta$ — некоторая фиксированная константа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте класс, осуществляющий сглаживание Лапласа. Он должен иметь аналогичный интерфейс, как и StraightforwardProbabilityEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LaplaceProbabilityEstimator:\n",
    "    \"\"\"Class for probability estimations of type P(word | context).\n",
    "    \n",
    "    P(word | context) = (c(context + word) + delta) / (c(context) + delta * V), where\n",
    "    c(sequence) - number of occurances of the sequence in the corpus,\n",
    "    delta - some constant,\n",
    "    V - number of different words in corpus.\n",
    "    \n",
    "    Args:\n",
    "        storage(NGramStorage): Object of NGramStorage class which will\n",
    "            be used to extract frequencies of ngrams.\n",
    "        delta(float): Smoothing parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage, delta=1.):\n",
    "        self.__storage = storage\n",
    "        self.__delta = delta\n",
    "        \n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) up to the length of max_n.\n",
    "        \"\"\"\n",
    "        if len(context) + 1 > self.__storage.max_n:\n",
    "            return context[-self.__storage.max_n + 1:]\n",
    "        return context\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(word, str):\n",
    "            raise TypeError('word must be a string!')\n",
    "        if not isinstance(context, tuple):\n",
    "            raise TypeError('context must be a tuple!')\n",
    "            \n",
    "        ### YOUR CODE HERE\n",
    "        # If context is too large, let's cut it.\n",
    "        context = self.cut_context(context)\n",
    "        phrase_counts = self.__storage(context + (word, ))\n",
    "        context_counts = self.__storage(context)\n",
    "        prob = (phrase_counts + self.__delta) / (context_counts + self.__delta * len(self.__storage[1]))\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберите наилучший параметр $\\delta$ для данного корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace estimator perplexity = 1705.7374873632343\n",
      "8.24753958945e-05\n"
     ]
    }
   ],
   "source": [
    "# Try to find out best delta parameter. We will not provide you any strater code.\n",
    "### YOUR CODE HERE\n",
    "def get_perplexity(delta):\n",
    "    return perplexity(LaplaceProbabilityEstimator(storage, delta), test_sents)\n",
    "\n",
    "deltas = np.linspace(0.0002, 0.0007, 100)\n",
    "perplexities = np.array([get_perplexity(delta) for delta in deltas])\n",
    "best_delta = deltas[np.argmin(perplexities)]\n",
    "### END YOUR CODE\n",
    "\n",
    "# Initialize estimator\n",
    "laplace_estimator = LaplaceProbabilityEstimator(storage, best_delta)\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Laplace estimator perplexity = {}'.format(perplexity(laplace_estimator, test_sents)))\n",
    "print(laplace_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fa6c5688b70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEKCAYAAADq59mMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGK9Z8TDGGOM1Kx7GGGO8ZsXDGGOM1/4/U++Q5tEk5DsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa6cd8f4ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plot\n",
    "%matplotlib inline\n",
    "\n",
    "# Рисуем, чтобы удостовериться, что выбран правильный интервал поиска дельты\n",
    "# Этот интервал искался руками.\n",
    "axis = plot.gca()\n",
    "plot.plot(deltas, perplexities)\n",
    "axis.set_xlabel('delta')\n",
    "axis.set_ylabel('perplexy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stupid backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **простого отката** довольно понятна. Если у нас есть достаточно информцаии для подсчета вероятности $k$-грам, то будем использовать $k$-грамы. Иначе будем использовать вероятности $(k-1)$-грам с некоторым множителем, например, $0.4$, и так далее. К сожалению, в данном случае мы получим не вероятностное распределение, но в большинстве задач это не имеет принципиального значения. Если это все же важно, то необходимо подобрать множитель соответствующим образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте класс, симулирующий сглаживание простым откатом. Он должен иметь аналогичный интерфейс, как и StraightforwardProbabilityEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StupidBackoffProbabilityEstimator:\n",
    "    \"\"\"Class for stupid backoff probability estimations.\n",
    "    \n",
    "    P(word | context) =\n",
    "        P'(word | context),                  if  P'(word | context) > 0;\n",
    "        P'(word | context[1:]) * multiplier, if  P'(word | context) == 0\n",
    "                                             and P'(word | context[1:]) > 0;\n",
    "        ...\n",
    "    P'(word | context) - probability of a word provided context of a base estimator.\n",
    "    \n",
    "    Args:\n",
    "        base_estimator(BaseProbabilityEstimator): Object of BaseProbabilityEstimator\n",
    "            or some other class which can estimate conditional probabilities.\n",
    "        multiplier (float): Multiplier which is used for probability estimations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_estimator, multiplier=0.1):\n",
    "        self.__base_estimator = base_estimator\n",
    "        self.__mult = multiplier\n",
    "\n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) by one word.\n",
    "        \"\"\"\n",
    "        if len(context) != 1:\n",
    "            return context[1:]\n",
    "        else:\n",
    "            return tuple()\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        context = self.__base_estimator.cut_context(context)\n",
    "        counter = 0\n",
    "        while self.__base_estimator(word, context) == 0:\n",
    "            context = self.cut_context(context)\n",
    "            counter += 1\n",
    "        prob = (self.__mult ** counter) * self.__base_estimator(word, context)\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stupid backoff estimator perplexity = 397.6440126849965\n",
      "8.24753958945e-05\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "sbackoff_estimator = StupidBackoffProbabilityEstimator(simple_estimator, .4)\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Stupid backoff estimator perplexity = {}'.format(perplexity(sbackoff_estimator, test_sents)))\n",
    "print(laplace_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на следующие вопросы (внутри ipython ноутбука):\n",
    "\n",
    "**Q:** Почему бессмысленно измерять перплексию в случае **Stupid backoff**?  \n",
    "**A:** Из-за тупого отнятия вероятности распределение перестает быть вероятностным (сумма всех вероятностей не единица). Перплексия, как вероятностная величина теряет значение.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае идея сглаживания посредством **интерполяции** также крайне проста. Пусть у нас есть $N$-грамная модель. Заведем вектор $\\bar\\lambda = (\\lambda_1, \\dots, \\lambda_N)$, такой, что $\\sum_i\\lambda_i = 1$ и $\\lambda_i \\geq 0$. Тогда\n",
    "\n",
    "$$\n",
    "    \\hat P_{IS}(w_{N} \\mid w_1^{N-1}) = \\sum_{i=1}^N \\lambda_i \\hat P_{S}(w_N \\mid w_{N-i+1}^{N-1}).\n",
    "$$\n",
    "\n",
    "Придумайте, как обойтись одним вектором $\\bar\\lambda$, т.е. пользоваться им как в случае контекста длины $N$, так и при контексте меньшей длины (например, в начале предложения). Если мы просто обрубим сумму, то у нас уже не будет вероятностное распределение, что, конечно же, плохо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InterpolationProbabilityEstimator:\n",
    "    \"\"\"Class for interpolation probability estimations.\n",
    "    \n",
    "    P(word | context) =\n",
    "        lambda_N * P'(word | context) +\n",
    "        lambda_{N-1} * P'(word | context[1:]) +\n",
    "        ... +\n",
    "        lambda_1 * P'(word)\n",
    "    P'(word | context) - probability of a word provided context of a base estimator.\n",
    "    \n",
    "    Args:\n",
    "        base_estimator(BaseProbabilityEstimator): Object of BaseProbabilityEstimator\n",
    "            or some other class which can estimate conditional probabilities.\n",
    "        lambdas (np.array[float]): Lambdas which are used for probability estimations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_estimator, lambdas):\n",
    "        self.lambdas = lambdas\n",
    "        self.__base_estimator = base_estimator\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        context = self.__base_estimator.cut_context(context)\n",
    "        def normalized(lambdas):\n",
    "            lambdas_sum = sum(lambdas)\n",
    "            return np.array([one_lambda / lambdas_sum for one_lambda in lambdas])\n",
    "        # Обрезаем лямбды в начале. Делаем так, чтобы их было на 1 больше, чем длина контекста.\n",
    "        current_lambdas = normalized(self.lambdas[(len(self.lambdas) - len(context) - 1):])\n",
    "        prob = np.sum([\n",
    "            one_lambda * self.__base_estimator(word, context[i:])\n",
    "            for i, one_lambda in enumerate(reversed(current_lambdas))\n",
    "        ])\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation estimator perplexity = 434.2744248779088\n",
      "0.00110967353942\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize estimator\n",
    "interpol_estimator = InterpolationProbabilityEstimator(simple_estimator, np.array([0.2, 0.2, 0.6]))\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Interpolation estimator perplexity = {}'.format(perplexity(interpol_estimator, test_sents)))\n",
    "print(interpol_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучить значения параметров $\\lambda$ можно с помощью EM-алгоритма, но мы не будем этого здесь делать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kneser-Ney smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея данного сглаживания заключается в том, что словам, которые участвуют в большом количестве контекстов, присваиваются большие вероятности, а те, которые используются в паре-тройке контекстов, получают маленькие вероятности. Формулы приведены на слайде 37 лекции.\n",
    "Реализуйте данный подход."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KneserNeyProbabilityEstimator:\n",
    "    \"\"\"Class for probability estimations of type P(word | context).\n",
    "    \n",
    "    P(word | context) = ...\n",
    "    \n",
    "    Args:\n",
    "        storage(NGramStorage): Object of NGramStorage class which will\n",
    "            be used to extract frequencies of ngrams.\n",
    "        delta(float): KneserNey parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage, delta=1.):\n",
    "        self.__storage = storage\n",
    "        self.__delta = delta\n",
    "        \n",
    "        # Для множества величин в формуле сдулаем предподсчет. Иначе время ряботы очень большое.\n",
    "        \n",
    "        self._unique_continuations_number = Counter()\n",
    "        for length in range(1, self.__storage.max_n):\n",
    "            for ngramm in self.__storage[length]:\n",
    "                self._unique_continuations_number[ngramm[:-1]] += 1\n",
    "        \n",
    "        self._continuations_number = Counter()\n",
    "        for length in range(1, self.__storage.max_n):\n",
    "            for ngramm in self.__storage[length]:\n",
    "                self._continuations_number[ngramm[:-1]] += self.__storage(ngramm)\n",
    "        \n",
    "        self._unique_bigramms_beginigs_number = Counter()\n",
    "        for bigramm in self.__storage[2]:\n",
    "            self._unique_bigramms_beginigs_number[bigramm[1]] += 1\n",
    "        \n",
    "        counter = 0\n",
    "        for sentence in self.__storage[2]:\n",
    "            counter += self.__storage[2][sentence]\n",
    "        self._bigramms_number = counter\n",
    "    \n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) up to the length of max_n.\n",
    "        \"\"\"\n",
    "        if len(context) + 1 > self.__storage.max_n:\n",
    "            context = context[-self.__storage.max_n + 1:]\n",
    "        return context\n",
    "\n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(word, str):\n",
    "            raise TypeError('word must be a string!')\n",
    "        if not isinstance(context, tuple):\n",
    "            raise TypeError('word must be a string!')\n",
    "        # If context is too large, let's cut it.\n",
    "        context = self.cut_context(context)\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # Используется модифицированный вариант формул.\n",
    "        \n",
    "        phrase_counts = self.__storage(context + (word, ))\n",
    "        context_continuations = self._continuations_number[context]\n",
    "        if context_continuations == 0:\n",
    "            prob = self(word, context[1:])\n",
    "        elif len(context) == 0:\n",
    "            # По сравнению с лекциями, изменено. Используются идем, похожие на сглаживание Лапласа.\n",
    "            # Позволяет сделать вероятности не присутствующих n-gramm не нулевыми\n",
    "            prob = (max(self._unique_bigramms_beginigs_number[word] - self.__delta, 0) / self._bigramms_number\n",
    "                    + self.__delta / self.__storage(()))\n",
    "        else:\n",
    "            prob = (\n",
    "                max(phrase_counts - self.__delta, 0) / context_continuations\n",
    "                + self.__delta * self._unique_continuations_number[context]\n",
    "                * self(word, context[1:]) / context_continuations\n",
    "            )\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).|\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple estimator perplexity = 697.2494983250439\n",
      "3.5719197932920255e-10\n",
      "7.707342196510895e-18\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "kn_estimator = KneserNeyProbabilityEstimator(storage)\n",
    "\n",
    "# Estimating perplexity\n",
    "print('Simple estimator perplexity = {}'.format(perplexity(kn_estimator, test_sents)))\n",
    "print(kn_estimator.prob('To be'.split()))\n",
    "print(kn_estimator.prob('To be or not to be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определение языка документа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Постановка задачи:**  \n",
    "Одна из задач, которая может быть решена при помощи языковых моделей $-$ **определение языка документа**. Реализуйте два классификатора для определения языка документа:\n",
    "1. Наивный классификатор, который будет учитывать частоты символов и выбирать язык текста по признаку: распределение частот символов \"наиболее похоже\" на распределение частот символов в выбранном языке.\n",
    "2. Классификатор на основе языковых моделей. Сами придумайте, как он должен работать.  \n",
    "_Подсказка_: лучше считать n-грамы не по словам, а по символам.\n",
    "\n",
    "---\n",
    "\n",
    "**Как представлены данные:**  \n",
    "Во всех текстовых файлах на каждой строчке записано отдельное предложение.\n",
    "1. В папке _data_ находятся две папки: _full_ и _plain_. В _full_ находятся тексты в той форме, что они были взяты из сети, в _plain_ находятся те же самые тексты, но с них сначала была снята диакритика, а затем русский и греческий тексты были транслитерованы в английский.\n",
    "2. В каждой из папок _full_ и _plain_ находятся папки _train_ и _test_.\n",
    "3. В _train_ находятся файлы с текстами с говорящими именами, например, _ru.txt_, _en.txt_.\n",
    "4. В _test_ находятся файлы _1.txt_, _2.txt_, $\\dots$ в которых хранятся тексты, язык которых нужно определить. В этой же папке находится файл _ans.csv_, в котором вы можете найти правильные ответы и проверить, насколько хорошо сработали Ваши алгоритмы.\n",
    "\n",
    "---\n",
    "\n",
    "**Что нужно сделать:**  \n",
    "Напишите два своих классификатора (которые описаны в постановке задачи) и получите максимально возможное accuracy на test-сете. Разрешается использовать только _train_ для обучения.\n",
    "\n",
    "---\n",
    "\n",
    "**В данном задании мы не предоставляем стартового кода!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функции загрузки тестовых данных\n",
    "\n",
    "def get_plain_test_words():\n",
    "    result = {}\n",
    "    for i in range(1, 241):\n",
    "        result[i] = []\n",
    "        with open('language_detection/plain/test/' + str(i) + '.txt') as file:\n",
    "            for line in file:\n",
    "                result[i].extend(line.strip()[:-1].lower().split(' '))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функции загрузки обучающих данных\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "def get_plain_train_words():\n",
    "    result = {}\n",
    "    plain_train_path = 'language_detection/plain/train/'\n",
    "    train_files = [file for file in listdir(plain_train_path) if file.endswith('.txt')]\n",
    "    for train_file in train_files:\n",
    "        result[train_file[:-4]] = []\n",
    "        with open(plain_train_path + train_file) as file:\n",
    "            for line in file:\n",
    "                result[train_file[:-4]].extend(line.strip()[:-1].lower().split(' '))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Используются данные (plain), где слова на разных языках пишутся с использованием одного алфавита.\n",
    "\n",
    "plain_test_words = get_plain_test_words()\n",
    "plain_train_words = get_plain_train_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_any_key(dictionary):\n",
    "    for key in dictionary:\n",
    "        return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наивный классификатор хранит текст как мешок слов. При поступлении неизвестного языка\n",
    "# сравниваем отличие частот (как сумму модулей разницы) посчитанных частотных наборов.\n",
    "# Выбираем язык с наименьшим отклонением.\n",
    "\n",
    "def frequencies_difference(frequencies_1, frequencies_2):\n",
    "    difference = 0\n",
    "    for letter in (frequencies_1.keys() | frequencies_2.keys()):\n",
    "        difference += abs(frequencies_1[letter] - frequencies_2[letter])\n",
    "    return difference\n",
    "\n",
    "class NaiveClassifier:\n",
    "    def __init__(self):\n",
    "        self.letters_frequencies = {}\n",
    "\n",
    "    def get_letters_counter(self, words):\n",
    "        counter = Counter([letter for word in words for letter in word if letter.isalpha()])\n",
    "        for letter in counter:\n",
    "            counter[letter] /= len(words)\n",
    "        return counter\n",
    "\n",
    "    def fit(self, language_name, words):\n",
    "        self.letters_frequencies[language_name] = self.get_letters_counter(words)\n",
    "\n",
    "    def predict(self, words):\n",
    "        result_language = get_any_key(self.letters_frequencies)\n",
    "        predicting_frequencies = self.get_letters_counter(words)\n",
    "        least_difference = frequencies_difference(predicting_frequencies, self.letters_frequencies[result_language])\n",
    "        for language in self.letters_frequencies:\n",
    "            new_difference = frequencies_difference(predicting_frequencies, self.letters_frequencies[language])\n",
    "            if new_difference < least_difference:\n",
    "                least_difference = new_difference\n",
    "                result_language = language\n",
    "        return result_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Выбираем язык с минимальной перплексией. При этом используем лучших эстиматор из тех, что рассмотрели ранее.\n",
    "# На вход вместо предложений даем слова. Таким образом каждая буква становится словом, и от n-грамм слов\n",
    "# осуществляется переход к n-граммам букв. Используются 4-граммы. В начало и конец слова добавляется спуциальный символ\n",
    "# Начала и конца предложений 'B' и 'E'\n",
    "\n",
    "class InterpolationClassifier:\n",
    "    def __init__(self):\n",
    "        self.storagies = {}\n",
    "    \n",
    "    def fit(self, language_name, words):\n",
    "        words = ['B' + word + 'E' for word in words]\n",
    "        simple_estimator = StraightforwardProbabilityEstimator(NGramStorage(words, 4))\n",
    "        self.storagies[language_name] = InterpolationProbabilityEstimator(simple_estimator, np.array([0.1, 0.1, 0.2, 0.6]))\n",
    "    \n",
    "    def predict(self, words):\n",
    "        words = ['B' + word + 'E' for word in words]\n",
    "        result_language = get_any_key(self.storagies)\n",
    "        least_perplexity = perplexity(self.storagies[result_language], words)\n",
    "        for language in self.storagies:\n",
    "            new_perplexity = perplexity(self.storagies[language], words)\n",
    "            if new_perplexity < least_perplexity:\n",
    "                least_perplexity = new_perplexity\n",
    "                result_language = language\n",
    "        return result_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функция считает долю правильных ответов, сравнивая их с записанными в специальном файле\n",
    "\n",
    "def get_plain_test_accuracy(predictions):\n",
    "    with open('language_detection/plain/test/ans.csv') as file:\n",
    "        answers = [line[-3:-1] for line in file]\n",
    "    return len([1 for answer, prediction in zip(answers, predictions) if answer == prediction]) / len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем более умный классификатор и делаемпредсказания.\n",
    "\n",
    "clever_classifier = InterpolationClassifier()\n",
    "for language in plain_train_words:\n",
    "    clever_classifier.fit(language, plain_train_words[language])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clever_predictions = [clever_classifier.predict(plain_test_words[i]) for i in range(1, 241)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9958333333333333"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_plain_test_accuracy(clever_predictions) # Классификатор совершил одну ошибку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "naive_classifier = NaiveClassifier()\n",
    "for language in plain_train_words:\n",
    "    naive_classifier.fit(language, plain_train_words[language])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_predictions = [naive_classifier.predict(plain_test_words[i]) for i in range(1, 241)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9916666666666667"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_plain_test_accuracy(naive_predictions) # Наивный классификатор сработал чуть хуже. Этого слудовало ожидать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если подбирать модели и парамерты в интерполировании, то можно доиться абсолютного качества на тесте. В целом вывод такой, что n-граммные модели не плохо справляются с задачей определения языка текста."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
